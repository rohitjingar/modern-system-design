# 37. Retry & Backoff Mechanisms

A retry is when you fail and try again. Backoff is waiting longer each time because you're not patient. Together they're like: fail, wait 1 second, fail, wait 2 seconds, fail, wait 4 seconds... Eventually you either succeed or give up. Most of the time you give up. But those retries saved your life 1% of the time, so you keep them. Welcome to distributed systems probability! ğŸ²â³

[â† Back to Main](../README.md) | [Previous: Circuit Breakers](36-circuit-breakers.md) | [Next: Disaster Recovery](38-disaster-recovery.md)

---

## ğŸ¯ Quick Summary

**Retries** automatically repeat failed operations. **Backoff** waits progressively longer between retries. Without: transient failures cause permanent errors. With: temporary glitches auto-recover, improves reliability. Types: exponential backoff (1s, 2s, 4s, 8s), linear (1s, 2s, 3s), fixed (1s, 1s, 1s). Jitter prevents thundering herd (all clients retry simultaneously). Critical for network reliability. Trade-off: increased latency on failure, potential duplicate operations.

Think of it as: **Retries = Give It Another Shot**

---

## ğŸŒŸ Beginner Explanation

### Transient vs Permanent Failures

```
TRANSIENT FAILURE (Temporary):

Database connection fails:
â”œâ”€ Cause: Network blip, momentary overload
â”œâ”€ Duration: 100ms to 5 seconds
â”œâ”€ Solution: Retry after short wait
â””â”€ Will likely succeed

Timeout:
â”œâ”€ Cause: Service briefly slow
â”œâ”€ Duration: Recovers naturally
â”œâ”€ Solution: Retry after backoff
â””â”€ Usually succeeds

Load spike:
â”œâ”€ Cause: Too many requests
â”œâ”€ Duration: 1-2 seconds
â”œâ”€ Solution: Back off and retry
â””â”€ Succeeds when load drops

Result: Automatic recovery! âœ…


PERMANENT FAILURE (Don't retry):

File not found:
â”œâ”€ Cause: 404 error (resource doesn't exist)
â”œâ”€ Solution: Don't retry (will always fail)
â””â”€ Retrying wastes time

Authentication failed:
â”œâ”€ Cause: Invalid credentials
â”œâ”€ Solution: Don't retry (credentials don't change)
â””â”€ Wasting retries

Invalid input:
â”œâ”€ Cause: Bad data
â”œâ”€ Solution: Don't retry (data doesn't change)
â””â”€ Stop immediately

Result: Fail fast! âœ…
```

### Backoff Strategies

```
NO BACKOFF (Immediate retry):

Fail, retry immediately
â”œâ”€ Retry 1: Fail
â”œâ”€ Retry 2: Fail
â”œâ”€ Retry 3: Fail (service was down whole time)
â””â”€ Wasted effort

Problem:
âŒ Hammers struggling service
âŒ Makes situation worse (adds load)
âŒ Service can't recover


FIXED BACKOFF (Same wait each time):

Fail, wait 1 second, retry:
â”œâ”€ Retry 1: Fail
â”œâ”€ Wait: 1 second
â”œâ”€ Retry 2: Fail
â”œâ”€ Wait: 1 second
â”œâ”€ Retry 3: Success!
â””â”€ Took 2 seconds total

Pros:
âœ… Simple
âœ… Predictable

Cons:
âŒ Doesn't adapt (always 1 second)
âŒ If problem lasts 5 seconds: Still fails


LINEAR BACKOFF (Increase by fixed amount):

Fail, wait 1s, fail, wait 2s, fail, wait 3s:
â”œâ”€ Retry 1: Fail
â”œâ”€ Wait: 1 second
â”œâ”€ Retry 2: Fail
â”œâ”€ Wait: 2 seconds
â”œâ”€ Retry 3: Fail
â”œâ”€ Wait: 3 seconds
â”œâ”€ Retry 4: Success!
â””â”€ Took 6 seconds total

Pattern: wait = attempt Ã— 1 second

Pros:
âœ… Gradually increases
âœ… Gives service recovery time

Cons:
âŒ Increases too slowly for long outages


EXPONENTIAL BACKOFF (Double each time):

Fail, wait 1s, fail, wait 2s, fail, wait 4s:
â”œâ”€ Retry 1: Fail
â”œâ”€ Wait: 1 second (2^0)
â”œâ”€ Retry 2: Fail
â”œâ”€ Wait: 2 seconds (2^1)
â”œâ”€ Retry 3: Fail
â”œâ”€ Wait: 4 seconds (2^2)
â”œâ”€ Retry 4: Fail
â”œâ”€ Wait: 8 seconds (2^3)
â”œâ”€ Retry 5: Success!
â””â”€ Took 15 seconds total

Pattern: wait = 2^attempt (with max cap, e.g., 60s)

Pros:
âœ… Grows quickly
âœ… Gives struggling service breathing room
âœ… Used by AWS, Google, Netflix

Cons:
âŒ Can wait very long (capped usually)


EXPONENTIAL BACKOFF WITH JITTER:

Problem without jitter:
â”œâ”€ 1000 clients all fail at T=0
â”œâ”€ All retry at T=1 second (synchronized!)
â”œâ”€ All fail together
â”œâ”€ All retry at T=3 seconds
â”œâ”€ All fail together (thundering herd!)

Solution: Add randomness
â”œâ”€ Retry 1: Wait 1s + random(0-1s) = 0.5-1.5s
â”œâ”€ Retry 2: Wait 2s + random(0-2s) = 1-4s
â”œâ”€ Retry 3: Wait 4s + random(0-4s) = 2-8s
â”œâ”€ All clients retry at different times
â””â”€ Service load spread out

Result: No thundering herd! âœ…
```

### Retry Logic

```
WHAT TO RETRY:

Transient (YES):
â”œâ”€ TimeoutException
â”œâ”€ ConnectionRefused
â”œâ”€ 500 Internal Server Error
â”œâ”€ 502 Bad Gateway
â”œâ”€ 503 Service Unavailable
â””â”€ Network timeouts

Permanent (NO):
â”œâ”€ 400 Bad Request
â”œâ”€ 401 Unauthorized
â”œâ”€ 403 Forbidden
â”œâ”€ 404 Not Found
â”œâ”€ Invalid input
â””â”€ Authentication errors


HOW MANY RETRIES:

Default: 3 retries
â”œâ”€ Retry 1: Fail
â”œâ”€ Retry 2: Fail
â”œâ”€ Retry 3: Fail
â””â”€ Give up

Better: Adaptive (based on error type)
â”œâ”€ Timeout: 5 retries (try harder for timeouts)
â”œâ”€ 503 (overloaded): 3 retries (service will recover)
â”œâ”€ 400 (bad request): 0 retries (won't change)
â””â”€ 5xx (server error): 3 retries (might recover)


IDEMPOTENCY (Critical!):

Problem without idempotency:
â”œâ”€ Call: "Transfer $100"
â”œâ”€ Server executes, returns 500 (error)
â”œâ”€ Client retries: "Transfer $100"
â”œâ”€ Server executes AGAIN ($200 transferred!)
â””â”€ Disaster!

Solution: Idempotent operations
â”œâ”€ Assign unique ID to request
â”œâ”€ Call: "Transfer $100" (id=abc123)
â”œâ”€ Server executes, stores id
â”œâ”€ Client retries: "Transfer $100" (id=abc123)
â”œâ”€ Server recognizes id, returns previous result
â””â”€ Still $100 transferred âœ…
```

---

## ğŸ”¬ Advanced Explanation

### Backoff Algorithms

```
EXPONENTIAL BACKOFF WITH JITTER (AWS Standard):

base_delay = 1 second
max_delay = 32 seconds

For each retry:
  attempt = current attempt number
  
  # Exponential
  delay = min(base_delay Ã— 2^attempt, max_delay)
  
  # Add jitter (randomness)
  jitter = random(0, delay)
  wait_time = delay + jitter
  
  # Wait and retry
  sleep(wait_time)
  attempt_request()

Example:
Attempt 1: wait = min(2, 32) + jitter = 2s Â± 0-2s = 0.5-4s
Attempt 2: wait = min(4, 32) + jitter = 4s Â± 0-4s = 1-8s
Attempt 3: wait = min(8, 32) + jitter = 8s Â± 0-8s = 2-16s
Attempt 4: wait = min(16, 32) + jitter = 16s Â± 0-16s = 8-32s
Attempt 5: wait = min(32, 32) + jitter = 32s Â± 0-32s = 16-64s


DECORRELATED JITTER (Even better):

Prevents too-short waits:

temp = base_delay
For each retry:
  sleep_time = min(cap, random(base_delay, temp Ã— 3))
  temp = sleep_time
  attempt_request()

Example:
Attempt 1: sleep = random(1, 3) = 1.5s
Attempt 2: sleep = min(cap, random(1, 4.5)) = 2.1s
Attempt 3: sleep = min(cap, random(1, 6.3)) = 5.8s
Attempt 4: sleep = min(32, random(1, 17.4)) = 12.3s

Benefits:
âœ… More even distribution
âœ… Avoids clustering
âœ… Better load spreading
```

### Retry Policies

```
RETRY-ABLE HTTP STATUS CODES:

429 Too Many Requests:
â”œâ”€ Meaning: Rate limited
â”œâ”€ Action: Backoff and retry
â”œâ”€ Backoff: Respect Retry-After header
â””â”€ Example: Wait 60 seconds

500 Internal Server Error:
â”œâ”€ Meaning: Server crashed/error
â”œâ”€ Action: Backoff and retry (might recover)
â”œâ”€ Backoff: Exponential
â””â”€ Rationale: Temporary issue

502 Bad Gateway:
â”œâ”€ Meaning: Load balancer/proxy issue
â”œâ”€ Action: Backoff and retry
â”œâ”€ Backoff: Exponential
â””â”€ Rationale: Upstream recovering

503 Service Unavailable:
â”œâ”€ Meaning: Server overloaded
â”œâ”€ Action: Backoff and retry
â”œâ”€ Backoff: Exponential + jitter (important!)
â””â”€ Rationale: Thundering herd prevention

504 Gateway Timeout:
â”œâ”€ Meaning: Request took too long
â”œâ”€ Action: Backoff and retry
â”œâ”€ Backoff: Exponential
â””â”€ Rationale: Server might be recovering


NON-RETRY-ABLE STATUS CODES:

400 Bad Request:
â””â”€ Client's fault, won't change

401 Unauthorized:
â””â”€ Auth failed, won't change

403 Forbidden:
â””â”€ Permission denied, won't change

404 Not Found:
â””â”€ Resource doesn't exist, won't change

405 Method Not Allowed:
â””â”€ Wrong HTTP method, won't change
```

### Retry Budgets

```
RETRY BUDGET (Prevent retry storms):

Without budget:
â”œâ”€ Every failure retries N times
â”œâ”€ 1000 failures Ã— 3 retries = 3000 extra requests
â”œâ”€ Overloads already struggling system!

With budget:
â”œâ”€ Allow X% of requests to retry
â”œâ”€ Example: 10% retry budget
â”œâ”€ For every 100 requests: 10 can retry
â”œâ”€ If more than 10: Skip retry (fail fast)

Calculation:
â”œâ”€ retry_count = 0
â”œâ”€ max_retries_per_minute = (success_rate Ã— total_rate Ã— retry_budget) Ã— 60
â”œâ”€ If retry_count < max: Retry
â”œâ”€ Else: Fail immediately

Benefits:
âœ… Prevents cascading overload
âœ… Protects system health
âœ… Balances reliability vs load
```

---

## ğŸ Python Code Example

### âŒ Without Retry & Backoff (Fail Immediately)

```python
# ===== WITHOUT RETRY & BACKOFF =====

import requests

def call_api(url):
    """Call API without retry"""
    
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        # Fail immediately
        print(f"Error: {e}")
        return None

# Problem:
# âŒ Network blip? Immediate failure
# âŒ Service temporarily down? Immediate failure
# âŒ Load spike? Immediate failure
# âŒ No automatic recovery
```

### âœ… With Retry & Fixed Backoff

```python
# ===== WITH RETRY & FIXED BACKOFF =====

import requests
import time

def call_api_with_retry(url, max_retries=3, backoff=1):
    """Call API with retry and fixed backoff"""
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=5)
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                # Last attempt failed
                print(f"Failed after {max_retries} attempts: {e}")
                return None
            
            # Wait before retrying
            print(f"Attempt {attempt + 1} failed, retrying in {backoff}s...")
            time.sleep(backoff)
    
    return None

# Benefits:
# âœ… Retries transient failures
# âœ… Waits between attempts
# âŒ Doesn't backoff (same wait each time)
```

### âœ… With Exponential Backoff + Jitter

```python
# ===== WITH EXPONENTIAL BACKOFF + JITTER =====

import requests
import time
import random

def call_api_exponential(url, max_retries=5, base_delay=1, max_delay=32):
    """Call API with exponential backoff and jitter"""
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=5)
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.RequestException as e:
            # Check if should retry
            if attempt == max_retries - 1:
                print(f"Failed after {max_retries} attempts")
                return None
            
            # Calculate exponential backoff with jitter
            delay = min(base_delay * (2 ** attempt), max_delay)
            jitter = random.uniform(0, delay)
            wait_time = delay + jitter
            
            print(f"Attempt {attempt + 1} failed, waiting {wait_time:.1f}s...")
            time.sleep(wait_time)
    
    return None

# Benefits:
# âœ… Exponential backoff (gives service time)
# âœ… Jitter (prevents thundering herd)
```

### âœ… Production Retry Handler (Idempotent + Smart)

```python
# ===== PRODUCTION RETRY HANDLER =====

import requests
import time
import random
import uuid
from typing import Callable, TypeVar, Optional

T = TypeVar('T')

class RetryConfig:
    """Configuration for retry logic"""
    
    def __init__(self,
                 max_retries=3,
                 base_delay=1,
                 max_delay=32,
                 jitter=True):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.jitter = jitter

class RetryHandler:
    """Retry handler with exponential backoff"""
    
    # Retry-able status codes
    RETRYABLE_STATUS_CODES = {408, 429, 500, 502, 503, 504}
    
    # Non-retry-able status codes
    NON_RETRYABLE_STATUS_CODES = {400, 401, 403, 404, 405}
    
    def __init__(self, config: RetryConfig):
        self.config = config
    
    def should_retry(self, error: Exception) -> bool:
        """Determine if error is retryable"""
        
        if isinstance(error, requests.exceptions.Timeout):
            return True  # Retry timeouts
        
        if isinstance(error, requests.exceptions.ConnectionError):
            return True  # Retry connection errors
        
        if isinstance(error, requests.exceptions.HTTPError):
            status_code = error.response.status_code
            
            if status_code in self.RETRYABLE_STATUS_CODES:
                return True
            
            if status_code in self.NON_RETRYABLE_STATUS_CODES:
                return False
            
            # Default: retry 5xx, don't retry 4xx
            return 500 <= status_code < 600
        
        return False  # Other errors: don't retry
    
    def calculate_backoff(self, attempt: int) -> float:
        """Calculate backoff time with jitter"""
        
        # Exponential: delay = base * 2^attempt
        delay = min(
            self.config.base_delay * (2 ** attempt),
            self.config.max_delay
        )
        
        # Add jitter
        if self.config.jitter:
            jitter = random.uniform(0, delay)
            return delay + jitter
        
        return delay
    
    def execute_with_retry(self,
                          func: Callable[..., T],
                          *args,
                          **kwargs) -> Optional[T]:
        """Execute function with retry logic"""
        
        request_id = str(uuid.uuid4())  # For idempotency
        
        for attempt in range(self.config.max_retries):
            try:
                # Add request ID for idempotency
                kwargs['request_id'] = request_id
                
                result = func(*args, **kwargs)
                
                if attempt > 0:
                    print(f"âœ“ Succeeded on attempt {attempt + 1}")
                
                return result
            
            except Exception as e:
                if not self.should_retry(e):
                    print(f"âœ— Non-retryable error: {e}")
                    raise
                
                if attempt == self.config.max_retries - 1:
                    print(f"âœ— Failed after {self.config.max_retries} attempts")
                    raise
                
                # Calculate backoff
                wait_time = self.calculate_backoff(attempt)
                
                print(f"âœ— Attempt {attempt + 1} failed, " \
                      f"retrying in {wait_time:.1f}s...")
                
                time.sleep(wait_time)
        
        return None

# Usage
def call_user_service(user_id, request_id=None):
    """Call user service with idempotent request ID"""
    
    headers = {}
    if request_id:
        headers['X-Idempotency-Key'] = request_id
    
    response = requests.get(
        f'http://user-service:8000/users/{user_id}',
        headers=headers,
        timeout=5
    )
    response.raise_for_status()
    return response.json()

# Create retry handler
config = RetryConfig(max_retries=5, base_delay=1)
retry_handler = RetryHandler(config)

# Execute with retry
try:
    user = retry_handler.execute_with_retry(call_user_service, user_id=123)
    print(f"Got user: {user}")
except Exception as e:
    print(f"Failed to get user: {e}")

# Benefits:
# âœ… Smart retry decision (retryable vs permanent errors)
# âœ… Exponential backoff with jitter
# âœ… Idempotency support (request IDs)
# âœ… Production-ready
```

---

## ğŸ’¡ Mini Project: "Build Retry System"

### Phase 1: Basic Retry â­

**Requirements:**
- Fixed backoff
- Retry N times
- Error classification
- Simple implementation

---

### Phase 2: Smart Backoff â­â­

**Requirements:**
- Exponential backoff
- Jitter
- Configurable limits
- Error-type specific policies

---

### Phase 3: Production (Full) â­â­â­

**Requirements:**
- Idempotency support
- Retry budgets
- Metrics tracking
- Circuit breaker integration

---

## âš–ï¸ Retry Strategies Comparison

| Strategy | Wait | Spread | Load | Complexity |
|----------|------|--------|------|-----------|
| **Immediate** | None | Clustered | High | Low |
| **Fixed** | 1s | Clustered | Medium | Low |
| **Linear** | 1s, 2s, 3s | Better | Medium | Medium |
| **Exponential** | 1s, 2s, 4s, 8s | Good | Low | Medium |
| **Exp + Jitter** | Randomized | Excellent | Very Low | Medium |

---

## âŒ Common Mistakes

### Mistake 1: Retrying Non-Retryable Errors

```python
# âŒ Retry everything
except Exception:
    retry()  # Retries 404 (won't help!)

# âœ… Only retry transient errors
except (TimeoutException, ConnectionError):
    retry()
except HTTPError as e:
    if e.response.status_code >= 500:
        retry()
```

### Mistake 2: No Idempotency

```python
# âŒ Retry without idempotency
call("Transfer $100")  # Fails
call("Transfer $100")  # RETRIED - charges $200!

# âœ… Use idempotency keys
call("Transfer $100", idempotency_key=uuid)
call("Transfer $100", idempotency_key=uuid)  # Returns cached result
```

### Mistake 3: Thundering Herd

```python
# âŒ All clients retry at same time
time.sleep(2)  # Fixed backoff
retry()

# âœ… Add jitter to spread out
time.sleep(2 + random.uniform(0, 2))
retry()
```

---

## ğŸ“š Additional Resources

**Retry Patterns:**
- [Exponential Backoff](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)
- [Retry Strategy](https://docs.microsoft.com/en-us/dotnet/architecture/microservices/implement-resilient-applications/implement-retries-exponential-backoff)

**Libraries:**
- [Tenacity (Python)](https://github.com/jd/tenacity)
- [Polly (.NET)](https://github.com/App-vNext/Polly)
- [Resilience4j (Java)](https://resilience4j.readme.io/)

---


## ğŸ¯ Before You Leave

**Can you answer these?**

1. **Transient vs permanent failure?**
   - Answer: Transient = retry; Permanent = fail fast

2. **Why exponential backoff?**
   - Answer: Gives service recovery time

3. **What's jitter?**
   - Answer: Randomness to prevent thundering herd

4. **Why idempotency?**
   - Answer: Safe to retry without side effects

5. **When to stop retrying?**
   - Answer: Max retries reached or non-retryable error

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **Client:** "I failed, let me retry!"
>
> **T=1s:** "Failed again, waiting..."
>
> **T=3s:** "Still failing, waiting longer..."
>
> **T=7s:** "Come on, service..."
>
> **T=15s:** "This is taking forever..."
>
> **T=31s:** "Did it work?"
>
> **Server:** "Yeah buddy, I'm back!"
>
> **Both:** "That was a journey" ğŸš€

---

[â† Back to Main](../README.md) | [Previous: Circuit Breakers](36-circuit-breakers.md) | [Next: Disaster Recovery](38-disaster-recovery.md)

---

**Last Updated:** November 11, 2025  
**Difficulty:** â­â­â­ Intermediate-Advanced (resilience)  
**Time to Read:** 25 minutes  
**Time to Implement:** 4-6 hours per phase  

---

*Retry & Backoff: The art of being patient when systems fail, which is always.* ğŸš€