# 38. Disaster Recovery

Disaster recovery is preparing for the worst. You write detailed plans for recovering from fires, floods, earthquakes, nuclear war, and data centers catching on fire. Then a developer accidentally runs `rm -rf /` in production and everything burns down anyway. All that planning and it was the simple stuff that got you. Welcome to reality! ğŸ”¥ğŸ’¾

[â† Back to Main](../README.md) | [Previous: Retry & Backoff Mechanisms](37-retry-backoff.md) | [Next: Blue-Green & Canary Deployments](39-blue-green-canary.md)

---

## ğŸ¯ Quick Summary

**Disaster Recovery (DR)** prepares systems to recover from catastrophic failures (data center down, data corruption, all servers dead). Metrics: RTO (recovery time objective: how fast?), RPO (recovery point objective: how much data loss?). Strategies: backups (periodic snapshots), replicas (real-time copies), multi-region (separate data centers). Netflix spans regions automatically. Amazon has 99.99% SLA (52.6 minutes downtime/year). Trade-off: cost (replication expensive), complexity (multi-region hard), latency (may increase).

Think of it as: **Disaster Recovery = Insurance Policy**

---

## ğŸŒŸ Beginner Explanation

### Disaster Scenarios

```
SCENARIO 1: Data Center Fire

T=0: Fire starts in Amazon Virginia data center
â”œâ”€ All servers gone
â”œâ”€ All data deleted
â”œâ”€ All customers: ERRORS
â””â”€ System down

Without DR:
â”œâ”€ No backup
â”œâ”€ No replicas elsewhere
â”œâ”€ Weeks to recover
â””â”€ Data lost forever

With DR:
â”œâ”€ Replicas in AWS Ohio (500 miles away)
â”œâ”€ Auto-failover triggers
â”œâ”€ Traffic rerouted to Ohio
â”œâ”€ T=5 minutes: System back up
â””â”€ Zero data loss!


SCENARIO 2: Database Corruption

T=0: Bug writes garbage to database
â”œâ”€ Old data overwritten
â”œâ”€ Corrupted data replicated everywhere
â”œâ”€ Replication lag (5 seconds): Corruption spreads
â””â”€ All copies corrupted

Without DR:
â”œâ”€ No backup
â”œâ”€ Manual recovery from logs: 2 hours
â””â”€ Downtime: 2 hours

With DR:
â”œâ”€ Hourly backups retained
â”œâ”€ Detect corruption at T=30 minutes
â”œâ”€ Restore from backup: 15 minutes old
â”œâ”€ Data loss: 15 minutes of transactions
â””â”€ Downtime: 45 minutes


SCENARIO 3: Ransomware Attack

T=0: Attacker encrypts all data
â”œâ”€ Demand: $10 million
â”œâ”€ Data inaccessible
â””â”€ System down

Without DR:
â”œâ”€ No unencrypted backup
â”œâ”€ Must pay ransom (maybe)
â””â”€ Months to recover

With DR:
â”œâ”€ Backup isolated from network (air-gapped)
â”œâ”€ Attacker can't access backup
â”œâ”€ Restore from backup: 2 hours
â”œâ”€ Don't pay ransom!
â””â”€ Back to normal
```

### DR Metrics

```
RTO (Recovery Time Objective):
"How long can we be down?"

Low RTO (< 1 hour):
â”œâ”€ Critical systems
â”œâ”€ Requires: Hot standby, auto-failover
â”œâ”€ Cost: Very expensive (2x infrastructure)
â””â”€ Example: Banking, hospitals

Medium RTO (1-4 hours):
â”œâ”€ Important systems
â”œâ”€ Requires: Warm standby, manual failover
â”œâ”€ Cost: Expensive (1.5x infrastructure)
â””â”€ Example: SaaS platforms

High RTO (> 4 hours):
â”œâ”€ Non-critical systems
â”œâ”€ Requires: Cold standby, manual restore
â”œâ”€ Cost: Moderate (regular backups)
â””â”€ Example: Internal tools


RPO (Recovery Point Objective):
"How much data loss acceptable?"

Zero RPO:
â”œâ”€ No data loss
â”œâ”€ Requires: Synchronous replication
â”œâ”€ Cost: Very expensive (high latency)
â”œâ”€ Example: Financial transactions

Minutes RPO (5-60 min):
â”œâ”€ Small data loss
â”œâ”€ Requires: Regular backups + replication
â”œâ”€ Cost: Expensive
â””â”€ Example: E-commerce

Hours RPO (1-24 hours):
â”œâ”€ Moderate data loss
â”œâ”€ Requires: Daily backups
â”œâ”€ Cost: Moderate
â””â”€ Example: Analytics


FORMULA:
Downtime cost = (Hourly revenue loss) Ã— (Hours down)

Example:
â”œâ”€ Revenue: $1M/hour
â”œâ”€ Downtime: 4 hours (no DR)
â”œâ”€ Cost: $4M
â”œâ”€ DR infrastructure: $500k/year
â””â”€ Payback: 1 major disaster = pays for itself!
```

### Backup Strategies

```
BACKUP TYPES:

Full Backup:
â”œâ”€ Copy all data
â”œâ”€ Size: Large (100GB+)
â”œâ”€ Time: Slow (hours)
â”œâ”€ Storage: Expensive
â””â”€ Frequency: Weekly

Example:
â”œâ”€ Size: 1TB database
â”œâ”€ Full backup: 1TB stored
â””â”€ Once/week: 52TB/year


Incremental Backup:
â”œâ”€ Copy only changed data
â”œâ”€ Size: Small (10GB+)
â”œâ”€ Time: Fast (minutes)
â”œâ”€ Storage: Cheap
â””â”€ Frequency: Daily

Example:
â”œâ”€ Database: 1TB
â”œâ”€ Daily incremental: 10GB/day
â”œâ”€ Weekly: 70GB
â””â”€ 7 incrementals + 1 full backup


Differential Backup:
â”œâ”€ Copy changes since last full
â”œâ”€ Size: Medium (50GB)
â”œâ”€ Time: Medium (30 min)
â”œâ”€ Frequency: Daily

Restore strategy:
â”œâ”€ Full backup (1TB)
â”œâ”€ Plus latest differential (50GB)
â””â”€ Fast restore (don't need all incrementals)


BACKUP LOCATIONS:

Local Backup:
â”œâ”€ Same data center
â”œâ”€ Speed: Fast (near local)
â”œâ”€ Cost: Cheap
â””â”€ Risk: Fire destroys backup too!

Regional Backup:
â”œâ”€ Different city (100 miles)
â”œâ”€ Speed: Slower (network transfer)
â”œâ”€ Cost: Moderate
â””â”€ Safety: Survives local disaster

Cross-Region Backup:
â”œâ”€ Different country (1000s miles)
â”œâ”€ Speed: Very slow
â”œâ”€ Cost: Expensive
â””â”€ Safety: Survives regional catastrophe

Rule of 3-2-1:
â”œâ”€ 3 copies of data
â”œâ”€ On 2 different media types
â”œâ”€ 1 copy offsite
â””â”€ Survives most disasters
```

---

## ğŸ”¬ Advanced Explanation

### DR Architectures

```
ACTIVE-PASSIVE (Failover):

Active data center (Primary):
â”œâ”€ Handles all traffic
â”œâ”€ Writes to local database
â”œâ”€ Replicates to passive
â””â”€ Region: Virginia

Passive data center (Standby):
â”œâ”€ Receives replicated data
â”œâ”€ Idle, waiting
â”œâ”€ Keeps database in sync
â””â”€ Region: Ohio

Disaster strikes Virginia:
â”œâ”€ Health checks fail
â”œâ”€ DNS switches to Ohio
â”œâ”€ Ohio becomes active
â”œâ”€ Traffic flows to Ohio
â””â”€ Recovery time: 5-10 minutes

Data loss:
â”œâ”€ Async replication lag: 5 seconds
â”œâ”€ Worst case: 5 seconds lost
â””â”€ RPO: 5 seconds


ACTIVE-ACTIVE (Multi-Region):

Active data center 1 (Virginia):
â”œâ”€ Handles traffic
â”œâ”€ Writes to database
â””â”€ Replicates to DC2

Active data center 2 (Ohio):
â”œâ”€ Handles traffic
â”œâ”€ Writes to database
â””â”€ Replicates to DC1

Both serving traffic simultaneously:
â”œâ”€ Traffic split 50-50
â”œâ”€ Both writing
â”œâ”€ Both replicating
â””â”€ Conflict resolution needed

Disaster strikes Virginia:
â”œâ”€ Ohio continues serving
â”œâ”€ No failover needed!
â”œâ”€ Recovery time: 0 seconds

Data loss:
â”œâ”€ Multi-master replication: Eventual consistency
â”œâ”€ Worst case: Data race/conflict
â””â”€ RPO: 0 seconds


BACKUP RECOVERY:

Database corrupted:
â”œâ”€ Detect corruption at T=30 min
â”œâ”€ Start restore from T=0 backup
â”œâ”€ Restore takes 2 hours
â”œâ”€ Recovery time: 2 hours
â””â”€ Data loss: 30 minutes (to corruption)


TIERED DR:

Tier 1 (Critical):
â”œâ”€ RTO: < 1 hour
â”œâ”€ Requires: Active-active or hot standby
â”œâ”€ Example: Payments, auth

Tier 2 (Important):
â”œâ”€ RTO: < 4 hours
â”œâ”€ Requires: Warm standby
â”œâ”€ Example: Orders, inventory

Tier 3 (Nice-to-have):
â”œâ”€ RTO: < 24 hours
â”œâ”€ Requires: Daily backups
â”œâ”€ Example: Analytics, reports
```

### Chaos Engineering (DR Testing)

```
PROBLEM: DR untested

Disaster happens:
â”œâ”€ Recovery plan exists
â”œâ”€ But nobody tested it
â”œâ”€ Steps outdated
â”œâ”€ Tools don't exist
â”œâ”€ Panic ensues
â””â”€ Recovery fails

SOLUTION: Chaos Engineering

Regularly test DR:
â”œâ”€ Kill random servers
â”œâ”€ Simulate network partition
â”œâ”€ Delete databases
â”œâ”€ Trigger backups
â””â”€ Practice recovery

Netflix's Chaos Monkey:
â”œâ”€ Randomly kills instances in production
â”œâ”€ Developers must handle it
â”œâ”€ Builds resilience culture
â”œâ”€ Discovered many issues

Example test:
â”œâ”€ Kill primary database
â”œâ”€ Time to failover: 3 minutes
â”œâ”€ Were alerts sent? Yes
â”œâ”€ Did backup work? Yes
â”œâ”€ Update DR plan: OK
â””â”€ Ready for real disaster!
```

---

## ğŸ Python Code Example

### âŒ Without Disaster Recovery

```python
# ===== WITHOUT DISASTER RECOVERY =====

import psycopg2

# Single database server
db = psycopg2.connect("dbname=shop host=db.primary.com")

def save_order(user_id, items):
    """Save order to single database"""
    
    cursor = db.cursor()
    cursor.execute("""
        INSERT INTO orders (user_id, items, created_at)
        VALUES (%s, %s, NOW())
        RETURNING id
    """, (user_id, str(items)))
    
    order_id = cursor.fetchone()[0]
    db.commit()
    
    return order_id

# Problem:
# âŒ Single database server
# âŒ No backups
# âŒ No replicas
# âŒ If datacenter burns: Data gone forever
# âŒ No recovery plan
```

### âœ… With Backup & Replication

```python
# ===== WITH BACKUP & REPLICATION =====

import psycopg2
import psycopg2.pool
from datetime import datetime
import subprocess

# Primary database (Virginia)
primary_pool = psycopg2.pool.SimpleConnectionPool(
    1, 10,
    "dbname=shop host=db.primary.us-east-1.com user=app"
)

# Replica database (Ohio)
replica_pool = psycopg2.pool.SimpleConnectionPool(
    1, 10,
    "dbname=shop host=db.replica.us-west-1.com user=app"
)

class BackupManager:
    """Manage backups and replication"""
    
    def __init__(self):
        self.primary_pool = primary_pool
        self.replica_pool = replica_pool
    
    def save_order(self, user_id, items):
        """Save order to primary (auto-replicated)"""
        
        conn = self.primary_pool.getconn()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO orders (user_id, items, created_at)
                VALUES (%s, %s, NOW())
                RETURNING id
            """, (user_id, str(items)))
            
            order_id = cursor.fetchone()[0]
            conn.commit()
            
            # Replication happens automatically
            # (PostgreSQL streams WAL to replica)
            
            return order_id
        
        finally:
            self.primary_pool.putconn(conn)
    
    def create_backup(self):
        """Create point-in-time backup"""
        
        timestamp = datetime.utcnow().isoformat()
        backup_dir = f"/backups/shop-{timestamp}"
        
        print(f"Creating backup: {backup_dir}")
        
        # Use pg_basebackup
        result = subprocess.run([
            'pg_basebackup',
            '-h', 'db.primary.us-east-1.com',
            '-D', backup_dir,
            '-Ft',  # Tar format
            '-z',   # Compressed
            '-Pv'   # Verbose progress
        ], capture_output=True)
        
        if result.returncode == 0:
            print(f"âœ“ Backup created: {backup_dir}")
            
            # Copy to S3 (remote storage)
            subprocess.run([
                'aws', 's3', 'cp',
                backup_dir,
                f's3://backups-prod/shop-{timestamp}/',
                '--recursive'
            ])
            
            print(f"âœ“ Backup uploaded to S3")
            return backup_dir
        
        else:
            print(f"âœ— Backup failed: {result.stderr}")
            return None
    
    def verify_replication(self):
        """Check replication lag"""
        
        # Get primary LSN (log sequence number)
        primary_conn = self.primary_pool.getconn()
        try:
            cursor = primary_conn.cursor()
            cursor.execute("SELECT pg_current_wal_lsn()")
            primary_lsn = cursor.fetchone()[0]
        finally:
            self.primary_pool.putconn(primary_conn)
        
        # Get replica LSN
        replica_conn = self.replica_pool.getconn()
        try:
            cursor = replica_conn.cursor()
            cursor.execute("SELECT pg_last_xact_replay_timestamp()")
            replica_timestamp = cursor.fetchone()[0]
        finally:
            self.replica_pool.putconn(replica_conn)
        
        print(f"Primary LSN: {primary_lsn}")
        print(f"Replica LSN timestamp: {replica_timestamp}")
        
        return True
    
    def promote_replica(self):
        """Promote replica to primary (manual failover)"""
        
        print("Promoting replica to primary...")
        
        replica_conn = self.replica_pool.getconn()
        try:
            cursor = replica_conn.cursor()
            cursor.execute("SELECT pg_promote()")
            replica_conn.commit()
            
            print("âœ“ Replica promoted to primary")
            return True
        
        finally:
            self.replica_pool.putconn(replica_conn)
    
    def restore_from_backup(self, backup_path):
        """Restore database from backup"""
        
        print(f"Restoring from backup: {backup_path}")
        
        # Extract backup
        result = subprocess.run([
            'tar', '-xzf', f'{backup_path}/base.tar.gz',
            '-C', '/var/lib/postgresql/new_cluster'
        ], capture_output=True)
        
        if result.returncode == 0:
            print("âœ“ Backup restored")
            return True
        
        else:
            print(f"âœ— Restore failed: {result.stderr}")
            return False

# Usage
backup_mgr = BackupManager()

# Regular operations
order_id = backup_mgr.save_order(user_id=123, items=['item1', 'item2'])
print(f"Order created: {order_id}")

# Daily backup
backup_mgr.create_backup()

# Verify replication
backup_mgr.verify_replication()

# In case of disaster:
# backup_mgr.promote_replica()  # Failover
# backup_mgr.restore_from_backup("/path/to/backup")  # Restore

# Benefits:
# âœ… Replicated to another region
# âœ… Daily backups to S3
# âœ… Can failover automatically
# âœ… Can restore from backup
```

### âœ… Production DR (Complete Strategy)

```python
# ===== PRODUCTION DR STRATEGY =====

from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import boto3

@dataclass
class DRMetrics:
    """DR metrics tracking"""
    rto_minutes: int  # Recovery time objective
    rpo_minutes: int  # Recovery point objective
    last_backup: datetime
    replication_lag: float
    backup_verified: bool

class DisasterRecoveryManager:
    """Production-grade DR management"""
    
    def __init__(self):
        self.s3 = boto3.client('s3')
        self.cloudwatch = boto3.client('cloudwatch')
        self.metrics = {}
    
    def get_metrics(self) -> DRMetrics:
        """Get current DR metrics"""
        
        return DRMetrics(
            rto_minutes=5,  # Can failover in 5 min
            rpo_minutes=1,  # Max 1 min data loss
            last_backup=datetime.utcnow() - timedelta(hours=1),
            replication_lag=0.5,  # 500ms lag
            backup_verified=True
        )
    
    def failover_to_region(self, target_region: str):
        """Automatic failover to another region"""
        
        print(f"Initiating failover to {target_region}...")
        
        # 1. Promote replica
        print("1. Promoting replica...")
        
        # 2. Update DNS
        print("2. Updating DNS...")
        
        # 3. Update service discovery
        print("3. Updating service discovery...")
        
        # 4. Verify traffic flowing
        print("4. Verifying traffic...")
        
        print(f"âœ“ Failover to {target_region} complete")
    
    def verify_dr_readiness(self) -> bool:
        """Verify DR plan is ready"""
        
        checks = {
            'backups_recent': self._check_backups(),
            'replication_lag': self._check_replication_lag(),
            'failover_tested': self._check_failover_tested(),
            'runbook_updated': self._check_runbook(),
        }
        
        all_pass = all(checks.values())
        
        print("DR Readiness Check:")
        for check, result in checks.items():
            status = "âœ“" if result else "âœ—"
            print(f"  {status} {check}")
        
        return all_pass
    
    def _check_backups(self) -> bool:
        """Verify backups exist and are recent"""
        # Implementation
        return True
    
    def _check_replication_lag(self) -> bool:
        """Verify replication lag acceptable"""
        # Implementation
        return True
    
    def _check_failover_tested(self) -> bool:
        """Verify failover was tested recently"""
        # Implementation
        return True
    
    def _check_runbook(self) -> bool:
        """Verify runbook is up to date"""
        # Implementation
        return True

# Usage
dr_mgr = DisasterRecoveryManager()

# Check readiness
dr_mgr.verify_dr_readiness()

# Get metrics
metrics = dr_mgr.get_metrics()
print(f"RTO: {metrics.rto_minutes} minutes")
print(f"RPO: {metrics.rpo_minutes} minutes")

# In case of disaster
dr_mgr.failover_to_region("us-west-1")

# Benefits:
# âœ… DR plan verified
# âœ… Metrics tracked
# âœ… Automatic failover ready
# âœ… Production-ready
```

---

## ğŸ’¡ Mini Project: "Build DR System"

### Phase 1: Backups â­

**Requirements:**
- Daily database backups
- Upload to S3
- Verify backups
- Track backup schedule

---

### Phase 2: Replication â­â­

**Requirements:**
- Setup replica database
- Monitor replication lag
- Manual failover process
- Test failover

---

### Phase 3: Automated DR â­â­â­

**Requirements:**
- Automatic failover
- Multi-region setup
- Chaos engineering tests
- Full runbook

---

## âš–ï¸ DR Strategy Comparison

| Strategy | RTO | RPO | Cost | Complexity |
|----------|-----|-----|------|-----------|
| **Backups only** | Hours | Hours | Low | Low |
| **Backup + Replication** | Minutes | Minutes | Medium | Medium |
| **Hot standby** | Seconds | 0 | High | High |
| **Active-active** | 0 | 0 | Very High | Very High |

---

## âŒ Common Mistakes

### Mistake 1: Never Testing DR

```python
# âŒ Create backup, never test restore
backup_created = True
# But: Does restore actually work?
# Find out during real disaster: Too late!

# âœ… Test restore regularly
backup_mgr.create_backup()
backup_mgr.test_restore()  # Actually restore
backup_mgr.verify_data()   # Verify integrity
```

### Mistake 2: Backups in Same Region

```python
# âŒ Backup in Virginia, data center burns
backup = create_backup("us-east-1")
# Fire destroys both primary and backup!

# âœ… Backup in different region
backup = create_backup("us-east-1")
upload_to_s3_region("us-west-1")  # Cross-region backup
```

### Mistake 3: Outdated Runbook

```python
# âŒ Runbook written 2 years ago
# Tools changed
# Passwords changed
# Team structure changed
# Runbook useless during disaster!

# âœ… Update runbook regularly
# Test recovery procedures
# Keep passwords current
# Document recent changes
```

---

## ğŸ“š Additional Resources

**DR Planning:**
- [AWS Disaster Recovery](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/)
- [DR Best Practices](https://www.gartner.com/en/information-technology/glossary/disaster-recovery-dr)

**Chaos Engineering:**
- [Chaos Monkey (Netflix)](https://github.com/Netflix/chaosmonkey)
- [Gremlin (Chaos as a Service)](https://www.gremlin.com/)


---

## ğŸ¯ Before You Leave

**Can you answer these?**

1. **What's RTO vs RPO?**
   - Answer: RTO = time to recover; RPO = data loss acceptable

2. **Backup strategies?**
   - Answer: Full, incremental, differential

3. **Active-passive vs active-active?**
   - Answer: Passive = one way failover; Active = both running

4. **Why test DR?**
   - Answer: Real disaster won't be like your assumptions

5. **What's chaos engineering?**
   - Answer: Intentionally break things to find weaknesses

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **Plan:** "In case of disaster, we'll recover in 4 hours"
>
> **Reality:** Fire starts
>
> **Team:** "Where's the runbook?"
>
> **Someone:** "It's in the burning data center"
>
> **Everyone:** "OK, we'll wing it"
>
> **4 hours later:** "We're back online!"
>
> **Everyone:** "That was lucky, let's write that runbook now"

---

[â† Back to Main](../README.md) | [Previous: Retry & Backoff Mechanisms](37-retry-backoff.md) | [Next: Blue-Green & Canary Deployments](39-blue-green-canary.md)

---

**Last Updated:** November 11, 2025  
**Difficulty:** â­â­â­â­ Advanced (operational resilience)  
**Time to Read:** 26 minutes  
**Time to Implement:** 8-12 hours per phase  

---

*Disaster Recovery: Hope for the best, prepare for the worst, expect the unexpected.* ğŸš€