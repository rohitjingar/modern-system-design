# 22. Rate Limiting & Throttling

Rate limiting is your bouncer saying "Sorry, we're at capacity." Throttling is saying "Slow down, you're going too fast." Together they're your system's way of saying "Please stop breaking me." Developers ignore both and then wonder why their API melts. ğŸš«

[â† Back to Main](../README.md) | [Previous: Distributed Logging](21-distributed-logging.md) | [Next: API Gateways â†’](23-api-gateways.md)

---

## ğŸ¯ Quick Summary

**Rate Limiting & Throttling** protect systems from overload: rate limiting blocks requests exceeding a threshold, throttling delays them. Essential for APIs: prevent abuse, ensure fair resource sharing, maintain performance under load. Strategies: token bucket, sliding window, leaky bucket. Implemented at: API gateway, application, database levels. Without them: single malicious user can take down entire service. With them: graceful degradation under load.

Think of it as: **Rate Limiting = No Entry, Throttling = Slow Down**

---

## ğŸŒŸ Beginner Explanation

### The Concert Analogy

**WITHOUT RATE LIMITING:**

```
Concert venue (1000 capacity)

No door policy:
â”œâ”€ 5000 people try to enter
â”œâ”€ Everyone crowds the door
â”œâ”€ Stampede! ğŸ’¥
â”œâ”€ 50 people injured
â”œâ”€ Event cancelled
â””â”€ Everyone angry

Without limits: System overwhelmed, everyone suffers!
```

**WITH RATE LIMITING:**

```
Concert venue with bouncer:

Bouncer says:
â”œâ”€ "Sorry, we're full!"
â”œâ”€ Only 1000 people allowed
â”œâ”€ Excess wait outside
â”œâ”€ No stampede
â”œâ”€ Everyone inside enjoys concert
â””â”€ Queue outside moves as people leave

With limits: System healthy, fair experience!
```

### Rate Limiting vs Throttling

**RATE LIMITING (Reject/Block):**

```
API: "Max 1000 requests per hour"

Request 1-1000: OK âœ…
Request 1001: "429 Too Many Requests" âŒ
Request 1002: "429 Too Many Requests" âŒ

Behavior: Hard rejection
Cost: Immediate feedback, no buffering
Used: Protect from abuse, ensure SLA
```

**THROTTLING (Slow Down/Queue):**

```
API: "Process at max 100 requests/second"

Requests 1-100: Process immediately
Requests 101-200: Queue (wait)
Requests 201-300: Queue (wait)

Behavior: Delay instead of reject
Cost: Buffering, increases latency
Used: Gradual degradation, fair sharing
```

### Who Gets Limited?

```
GLOBAL LIMIT:
â”œâ”€ All users share 10,000 req/hour
â”œâ”€ One user burns through fast
â”œâ”€ Others starved
â””â”€ Problem: Fair sharing

PER-USER LIMIT:
â”œâ”€ Each user: 1000 req/hour
â”œâ”€ Power user: 1000 req/hour
â”œâ”€ Other users: 1000 req/hour each
â””â”€ Solution: Fair sharing

TIERED LIMITS:
â”œâ”€ Free tier: 100 req/hour
â”œâ”€ Pro tier: 10,000 req/hour
â”œâ”€ Enterprise: Unlimited
â””â”€ Solution: Monetization!
```

---

## ğŸ”¬ Advanced Explanation

### Rate Limiting Algorithms

**TOKEN BUCKET (Most Popular):**

```
How it works:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Bucket       â”‚
â”‚  Capacity: 100      â”‚
â”‚  Current: 85 tokens â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Refill rate: 10 tokens/second

Request arrives:
â”œâ”€ Cost: 1 token
â”œâ”€ If tokens available: Process request, remove token
â”œâ”€ If no tokens: Reject/queue request

Timeline:
â”œâ”€ T=0s: 85 tokens, request arrives
â”‚        Process (84 tokens left)
â”œâ”€ T=0.1s: 86 tokens (refilled)
â”œâ”€ T=1s: 95 tokens (refilled 10)
â”œâ”€ T=1.1s: 94 tokens (request processed)

Benefits:
âœ… Handles bursts (capacity buffer)
âœ… Smooth rate limiting
âœ… Fair distribution
âœ… Simple to implement

Cost: Requires background refill
```

# SLIDING WINDOW

Track requests in **the last 1 hour (past only)**

---

### **Initial State**

Window size: **1 hour**
Limit: **1000 requests/hour**

```
Current Time = 19:00
Window = 18:00 â†’ 19:00 (last 1 hour)
Requests in this window = 850


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   18:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 19:00     â”‚
â”‚   [#########.............]        â”‚
â”‚   Count = 850 requests            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Still under 1000 â†’ all good.

---

# â­ **New Request at 18:50 (Correct Handling)**

At time **18:50**, window becomes:

```
Window = 17:50 â†’ 18:50
```

We drop all requests older than 17:50.

Letâ€™s say the user made:

* 700 requests before 17:50 â†’ OUTSIDE window
* 150 requests between 17:50â€“18:50 â†’ INSIDE window

So:

```
Request count in current window = 150
Limit = 1000 â†’ OK
Allow request
```

âœ” **Request at 18:50 = ALLOWED**

---

# â­ **New Request at 19:50 (Correct Handling)**

At time **19:50**, window becomes:

```
Window = 18:50 â†’ 19:50
```

All requests before 18:50 drop out.

Letâ€™s say:

* 250 requests happened between 18:50â€“19:50

So:

```
Request count = 250
Limit = 1000 â†’ OK
Allow request
```

âœ” **Request at 19:50 = ALLOWED**

---

Sliding Window ALWAYS checks:

```
Window = NOW âˆ’ 1 hour â†’ NOW
```

Never NOW â†’ NOW + 1 hour
Never future timestamps
Never shifts forward

---

# âš ï¸ **Issues with Sliding Window**

* Must store ALL request timestamps â†’ high memory usage
* Boundary situations cause uneven limiting
* Does NOT protect against short bursts (spikes)


---


# LEAKY BUCKET

Imagine you have a **bucket with a small hole at the bottom**.

* Water = **incoming requests**
* Hole = **processing speed**
* Bucket size = **maximum buffer**

Water can pour in **fast**,
but it only leaks out **at a fixed, constant rate**.

If too much water pours in and bucket overflows â†’ **extra water is rejected**.

This is EXACTLY how Leaky Bucket works.



**Leaky Bucket Diagram Explained**

```
Requests pouring in (fast)
     â†“  â†“  â†“  â†“  â†“ 
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BUCKET              â”‚
â”‚  ~~~~~~~~ requests      â”‚  <-- fills up quickly
â”‚  ~~~~~~~~               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ leak
            â†“
         100 req/sec
```



# â­ **Given Example:**

### **Requests coming in**: 1000 per second

(very high, crazy speed)

### **Bucket capacity**: 1000

(max number of requests we can temporarily store)

### **Leak rate**: 100 per second

(max processing speed â€” fixed)



# â­ **What happens step-by-step:**

### ğŸ”µ Step 1: Requests pour in fast (1000/sec)

They fill up the bucket.

### ğŸŸ¢ Step 2: The bucket leaks at a steady rate (100/sec)

Processing is **smooth**, not jumpy.

### ğŸ”´ Step 3: Bucket gets full (capacity = 1000)

Any request beyond capacity â†’ **rejected** immediately.



# ğŸ§  **Why does Leaky Bucket exist?**

Because systems MUST keep **output rate stable**.

Imagine:

* Server can only handle 100 requests/sec
* But users can send 1000 requests/sec

Without a bucket â†’ system crashes
With bucket â†’ system absorbs some burst



# ğŸï¸ **REAL-LIFE ANALOGY (PERFECT EXAMPLE)**

Imagine a toll booth on a highway:

* Cars arrive FAST (1000 cars/minute)
* Only **one gate**, allows 100 cars/minute â†’ fixed speed
* Cars queue up before the gate (the bucket)

If the queue becomes too long â†’ cars are turned away.

This is **Leaky Bucket**.



# âœ”ï¸ **OUTPUT RATE IS ALWAYS CONSTANT**

Even if input spikes randomly:

* 200/sec â†’ output stays 100/sec
* 5/sec â†’ output stays 100/sec (idle)
* 1000/sec â†’ output stays 100/sec

This prevents:

* DB overload
* API crashes
* Network congestion



# ğŸ”¥ **Where is Leaky Bucket used?**

Primarily in **network traffic shaping**, like:

* Routers
* Switches
* ISPs
* Linux kernel

Why?
Because network devices prefer **smooth, predictable traffic**.



# â­ **Leaky Bucket vs Token Bucket (Very Important Difference)**

| Feature        | Token Bucket  | Leaky Bucket    |
| -------------- | ------------- | --------------- |
| Allows bursts? | YES           | NO              |
| Output rate    | Variable      | Constant        |
| Good for       | Rate limiting | Traffic shaping |
| Used by        | APIs          | Networks        |

**Token Bucket = burst-friendly**
**Leaky Bucket = burst-smoothed**


# âœ”ï¸ **LEAKY BUCKET SUMMARY (memorize this)**

> **Requests enter bucket fast â†’ leak out slowly at a fixed rate.
> If bucket overflows â†’ new requests are rejected.**

---


# Where to Rate Limit

**API GATEWAY (Layer 1 - First Line):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway         â”‚
â”‚ Rate Limiter HERE   â”‚ â† Check quota
â”‚ â†’ Check: 1000/hour? â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ (only if within quota)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application Server  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Centralized control
âœ… Protects entire backend
âœ… Catches abuse early
âœ… Easy to modify limits

Examples: Nginx, AWS API Gateway
```

**APPLICATION LEVEL (Layer 2):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ (all requests)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application         â”‚
â”‚ Rate Limiter HERE   â”‚ â† Per-endpoint limits
â”‚ /api/search: 10/sec â”‚
â”‚ /api/upload: 1/sec  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Granular per-endpoint
âœ… Custom per-service logic
âœ… Can share state in memory

Examples: Guava RateLimiter, Bucket4j
```

**DATABASE LEVEL (Layer 3):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ (all queries)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database            â”‚
â”‚ Connection Pool: 10 â”‚ â† Limits concurrent
â”‚ Queue Size: 100     â”‚ â† Buffers excess
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Prevents DB overload
âœ… Automatic connection management
âœ… Queue for fairness

Examples: JDBC connection pool
```

### Quota Types

**HARD QUOTA (Reject):**

```
Limit: 1000 requests/hour

Request 1-1000: âœ… Success
Request 1001: âŒ 429 Too Many Requests

User hits limit: Completely blocked
Behavior: Strict
Used: Free tier, prevent abuse
```

**SOFT QUOTA (Alert):**

```
Limit: 1000 requests/hour
Alert at: 90% (900 requests)

900 requests: Warning email
  "You've used 90% of your quota"

1000 requests: Limit hit, further requests blocked

Used: Premium users, gradual degradation
```

**BURST QUOTA:**

```
Sustained: 100 req/sec
Burst: 500 req/sec for 10 seconds

Normal traffic: 100/sec
Traffic spike: Can go to 500/sec for 10s
After 10s: Back to 100/sec

Used: Handle traffic spikes fairly
```

### Distributed Rate Limiting

**PROBLEM: Multiple Servers**

```
Rate Limit: 1000 requests/hour

Server 1 (in-memory counter):
â”œâ”€ 500 requests counted

Server 2 (in-memory counter):
â”œâ”€ 500 requests counted

Load balancer routes randomly:
â”œâ”€ Client 1 â†’ Server 1 (counts)
â”œâ”€ Client 1 â†’ Server 2 (counts separately!)
â””â”€ Total: 1000 counted, but 1000 allowed on EACH!

Result: 2000 requests allowed (should be 1000!)
â”œâ”€ Rate limit bypassed! ğŸ˜±
â””â”€ System overloaded
```

**SOLUTION: Centralized Counter**

```
All servers share Redis counter:

Server 1: Check Redis (300), increment â†’ 301
Server 2: Check Redis (300), increment â†’ 302
Server 3: Check Redis (300), increment â†’ 303

Centralized Redis:
â””â”€ True count: 303 (correct!)

When count hits 1000:
â””â”€ All servers reject, regardless of which one receives request

Result:
âœ… Consistent limiting across all servers
âœ… True distributed rate limiting
```

---

## ğŸ Python Code Example

### âŒ Without Rate Limiting (Vulnerable)

```python
# ===== WITHOUT RATE LIMITING =====

from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/api/search', methods=['GET'])
def search():
    """Search API with no rate limiting"""
    
    query = request.args.get('q', '')
    
    # Expensive operation
    results = expensive_search(query)
    
    return jsonify(results)

def expensive_search(query):
    # Simulate complex search (1 second)
    time.sleep(1)
    return {"results": f"Results for {query}"}

# Problem:
# âŒ User sends 10,000 requests simultaneously
# âŒ All processed (server slows to crawl)
# âŒ Other users affected
# âŒ Malicious DDoS attack possible
```

### âœ… Simple Rate Limiting (Per-Process)

```python
# ===== SIMPLE RATE LIMITING (IN-MEMORY) =====

from flask import Flask, jsonify, request
from collections import defaultdict
import time

app = Flask(__name__)

class RateLimiter:
    """Simple token bucket rate limiter"""
    
    def __init__(self, rate: int, per: int = 60):
        """
        rate: max requests
        per: per N seconds
        """
        self.rate = rate
        self.per = per
        self.requests = defaultdict(list)
    
    def is_allowed(self, client_id: str) -> bool:
        """Check if request from client is allowed"""
        
        now = time.time()
        
        # Remove old requests (outside window)
        self.requests[client_id] = [
            req_time for req_time in self.requests[client_id]
            if now - req_time < self.per
        ]
        
        # Check if at limit
        if len(self.requests[client_id]) < self.rate:
            # Allow request
            self.requests[client_id].append(now)
            return True
        else:
            # Reject request
            return False

limiter = RateLimiter(rate=10, per=60)  # 10 req/min

@app.route('/api/search', methods=['GET'])
def search():
    """Search API with rate limiting"""
    
    client_id = request.remote_addr
    
    if not limiter.is_allowed(client_id):
        return jsonify({"error": "Rate limit exceeded"}), 429
    
    query = request.args.get('q', '')
    results = expensive_search(query)
    
    return jsonify(results)

# Benefits:
# âœ… Simple to implement
# âœ… Works per-server
# âœ… No external dependencies

# Problems:
# âŒ Only works on single server
# âŒ Memory grows with users
# âŒ Not distributed
```

### âœ… Production Rate Limiting (Distributed)

```python
# ===== PRODUCTION RATE LIMITING (REDIS) =====

import redis
import time
from typing import Tuple

class DistributedRateLimiter:
    """Redis-backed rate limiter (distributed)"""
    
    def __init__(self, redis_client, rate: int, per: int = 60):
        self.redis = redis_client
        self.rate = rate
        self.per = per
    
    def is_allowed(self, client_id: str) -> Tuple[bool, dict]:
        """Check if request allowed and return stats"""
        
        key = f"rate_limit:{client_id}"
        now = time.time()
        window = now - self.per
        
        # Remove old requests (before window)
        self.redis.zremrangebyscore(key, 0, window)
        
        # Count requests in window
        request_count = self.redis.zcard(key)
        
        if request_count < self.rate:
            # Allow request
            self.redis.zadd(key, {str(now): now})
            self.redis.expire(key, self.per)
            
            return True, {
                "limit": self.rate,
                "remaining": self.rate - request_count - 1,
                "reset_in": self.per
            }
        else:
            # Reject request
            oldest = float(self.redis.zrange(key, 0, 0, withscores=True)[0][1])
            reset_in = int(oldest + self.per - now)
            
            return False, {
                "limit": self.rate,
                "remaining": 0,
                "reset_in": reset_in
            }

# Usage
redis_client = redis.Redis(host='localhost', port=6379)
limiter = DistributedRateLimiter(
    redis_client,
    rate=100,  # 100 requests
    per=60     # per 60 seconds
)

@app.route('/api/search', methods=['GET'])
def search():
    """Search API with distributed rate limiting"""
    
    client_id = request.remote_addr
    allowed, stats = limiter.is_allowed(client_id)
    
    response_headers = {
        'X-RateLimit-Limit': str(stats['limit']),
        'X-RateLimit-Remaining': str(stats['remaining']),
        'X-RateLimit-Reset': str(int(time.time()) + stats['reset_in'])
    }
    
    if not allowed:
        return (
            jsonify({"error": "Rate limit exceeded"}),
            429,
            response_headers
        )
    
    query = request.args.get('q', '')
    results = expensive_search(query)
    
    return jsonify(results), 200, response_headers

# Benefits:
# âœ… Distributed across servers
# âœ… Consistent rate limiting
# âœ… Survives server restarts
# âœ… Includes response headers (client can see remaining quota)
```

### âœ… Advanced: Tiered Rate Limiting

```python
# ===== TIERED RATE LIMITING =====

class TieredRateLimiter:
    """Different limits for different user tiers"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        
        # Define tiers
        self.tiers = {
            'free': {'rate': 100, 'per': 3600},      # 100/hour
            'pro': {'rate': 10000, 'per': 3600},     # 10k/hour
            'enterprise': {'rate': float('inf'), 'per': 3600}  # Unlimited
        }
    
    def get_user_tier(self, user_id: str) -> str:
        """Get user's subscription tier"""
        # In real system: fetch from database
        return 'free'  # Default
    
    def is_allowed(self, user_id: str) -> Tuple[bool, dict]:
        """Check if request allowed"""
        
        tier = self.get_user_tier(user_id)
        limits = self.tiers[tier]
        
        # If unlimited: always allow
        if limits['rate'] == float('inf'):
            return True, {"tier": tier, "limit": "unlimited"}
        
        # Otherwise: standard rate limiting
        key = f"rate_limit:{user_id}"
        now = time.time()
        window = now - limits['per']
        
        # Count requests
        self.redis.zremrangebyscore(key, 0, window)
        request_count = self.redis.zcard(key)
        
        if request_count < limits['rate']:
            self.redis.zadd(key, {str(now): now})
            self.redis.expire(key, limits['per'])
            return True, {
                "tier": tier,
                "limit": limits['rate'],
                "remaining": limits['rate'] - request_count - 1
            }
        else:
            return False, {
                "tier": tier,
                "limit": limits['rate'],
                "remaining": 0
            }

# Usage
tiered_limiter = TieredRateLimiter(redis_client)

@app.route('/api/search', methods=['GET'])
def search():
    """Search with tiered rate limiting"""
    
    user_id = get_user_id(request)  # From auth
    allowed, stats = tiered_limiter.is_allowed(user_id)
    
    if not allowed:
        return jsonify({"error": "Rate limit exceeded"}), 429
    
    # Process request
    return jsonify({"results": "..."})

# Benefits:
# âœ… Monetization (free vs paid)
# âœ… Different limits per tier
# âœ… Enterprise: unlimited
```

---

## ğŸ’¡ Mini Project: "Build a Rate Limiter"

### Phase 1: Simple Algorithm â­

**Requirements:**
- Token bucket implementation
- Per-client limits
- In-memory storage
- Response headers
- Basic metrics

---

### Phase 2: Distributed â­â­

**Requirements:**
- Redis backend
- Multiple servers
- Consistent limits
- Tiered limits
- Request tracking

---

### Phase 3: Production â­â­â­

**Requirements:**
- Multiple algorithms (bucket, sliding window, leaky)
- Dynamic limit updates
- Analytics dashboard
- Abuse detection
- Graceful degradation

---

## âš–ï¸ Rate Limiting Algorithms Comparison

| Algorithm | Burst Handling | Complexity | Memory | Best For |
|-----------|---|---|---|---|
| **Token Bucket** | âœ… Yes | Low | Low | General purpose |
| **Sliding Window** | âŒ No | Medium | Medium | Strict limits |
| **Leaky Bucket** | âœ… Yes | Low | Low | Smooth output |
| **Fixed Window** | âŒ No | Very Low | Very Low | Simple |

---

## ğŸ¯ When to Use Rate Limiting

```
âœ… USE WHEN:
- Public APIs (prevent abuse)
- Limited resources (DB connections)
- Cost control (expensive operations)
- Fair resource sharing
- DDoS protection
- Free tier users
- Prevent runaway scripts

âŒ LESS CRITICAL WHEN:
- Internal APIs (trusted)
- Unlimited resources
- Single user (no contention)
```

---

## âŒ Common Mistakes

### Mistake 1: Only Global Limits

```python
# âŒ Single global limit
limit = 10000  # per hour total

# One user burns it all
# Others starved

# âœ… Per-user limits
limit_per_user = 1000  # per hour each
# Fair for all
```

### Mistake 2: Synchronous Rate Limiting

```python
# âŒ Check limit for every request (slow)
for request in incoming_requests:
    if check_limit_in_db(user):  # Database query!
        process(request)

# Performance dies under load

# âœ… Use local cache or Redis
if rate_limiter.is_allowed(user):
    process(request)  # In-memory, fast
```

### Mistake 3: No Response Headers

```python
# âŒ No indication of remaining quota
response = jsonify({"data": "..."})
return response

# User confused: how many left?
# Client can't optimize

# âœ… Include headers
response.headers['X-RateLimit-Remaining'] = str(remaining)
response.headers['X-RateLimit-Reset'] = str(reset_time)
return response
```

---

## ğŸ“š Additional Resources

**Libraries:**
- [Guava RateLimiter](https://guava.dev/) (Java)
- [Bucket4j](https://github.com/vladimir-bukhtoyarov/bucket4j) (Java)
- [Flask-Limiter](https://flask-limiter.readthedocs.io/) (Python)
- [express-rate-limit](https://github.com/nfriedly/express-rate-limit) (Node.js)

**Reading:**
- [API Rate Limiting](https://cloud.google.com/architecture/rate-limiting-strategies-techniques)
- [Token Bucket Algorithm](https://en.wikipedia.org/wiki/Token_bucket)



---

## ğŸ¯ Before You Leave

**Can you answer these?**

1. **What's the difference between rate limiting and throttling?**
   - Answer: Rate limiting rejects; throttling delays

2. **How does token bucket work?**
   - Answer: Refills tokens at fixed rate, reject when empty

3. **Why distributed rate limiting?**
   - Answer: Single server limits can be bypassed across multiple servers

4. **What response headers should you include?**
   - Answer: X-RateLimit-Limit, Remaining, Reset

5. **When to rate limit at API gateway vs application?**
   - Answer: Gateway for global; Application for per-endpoint

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **User:** "Why did my request get rejected?"
>
> **API:** "You exceeded rate limit."
>
> **User:** "But I only sent 1 request!"
>
> **API:** "Yeah, per microsecond." âš¡

---

[â† Back to Main](../README.md) | [Previous: Distributed Logging](21-distributed-logging.md) | [Next: API Gateways â†’](23-api-gateways.md)

---

**Last Updated:** November 11, 2025  
**Difficulty:** â­â­ Beginner-Intermediate (algorithm understanding)  
**Time to Read:** 24 minutes  
**Time to Build Limiter:** 3-6 hours per phase  

---

*Rate limiting: How to say "no more!" without breaking the API.* ğŸš€