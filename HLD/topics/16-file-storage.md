# 16. File Storage (S3, Blob Storage)

File storage is where you put stuff and hope you never have to retrieve it. But when you do, you find out you forgot the file path, the region, and the access key. Welcome to cloud purgatory. â˜ï¸ğŸ˜­

[â† Back to Main](../README.md) | [Previous: Data Modeling](15-data-modeling.md) | [Next: Load Balancers â†’](17-load-balancers.md)

---

## ğŸ¯ Quick Summary

**File Storage** (Object Storage) is how large files (images, videos, documents) are stored in the cloud. AWS S3, Azure Blob Storage, Google Cloud Storage are popular options. They're cheaper than databases for files, massively scalable, and handle millions of requests. Trade-off: higher latency than local disk, but unlimited scale and reliability.

Think of it as: **File Storage = Infinitely Large Hard Drive**

---

## ğŸŒŸ Beginner Explanation

### The Post Office Analogy

**LOCAL STORAGE (Hard Drive):**

```
Store files on your computer:

âœ… Fast (local disk)
âœ… No internet needed
âœ… No monthly cost
âŒ Limited space (1TB max usually)
âŒ If computer dies, files gone
âŒ Can't access from other places
âŒ Expensive to add more storage

Example: Your laptop hard drive
```

**FILE STORAGE (S3/Cloud):**

```
Store files in the cloud:

âœ… Unlimited space (literally infinite)
âœ… Access from anywhere
âœ… Automatic backups & redundancy
âœ… Cheap per GB ($0.023/GB for S3)
âœ… Handles millions of simultaneous requests
âœ… No server maintenance
âŒ Slightly higher latency (milliseconds)
âŒ Ongoing monthly cost
âŒ Network dependent

Example: S3, Google Cloud Storage, Azure Blob
```

### How File Storage Works

```
TRADITIONAL APPROACH (Bad):

Database stores file data:
â”‚
â”œâ”€ User uploads 10MB image
â”œâ”€ Database processes upload
â”œâ”€ Database stores as BLOB
â”œâ”€ Database gets HUGE (10MB Ã— 1M users = 10TB!)
â”œâ”€ Database becomes slow
â”œâ”€ Backups huge (10TB daily?)
â”œâ”€ Expensive!

Problem:
âŒ Database bloated
âŒ Backups slow
âŒ Performance hit
```

**FILE STORAGE APPROACH (Good):**

```
Database stores metadata, S3 stores file:

User uploads 10MB image:
â”‚
â”œâ”€ Database stores: filename, size, upload_date
â”œâ”€ S3 stores: actual file data
â”œâ”€ Database stays small
â”œâ”€ Backups fast (metadata only)
â”œâ”€ Both scale independently

Benefits:
âœ… Database small & fast
âœ… File storage unlimited
âœ… Separate scaling
âœ… Cheap!
```

### S3 Concepts

```
S3 STORAGE:

BUCKET:
â”œâ”€ Like a folder/directory
â”œâ”€ example.com-bucket-1
â”œâ”€ Contains objects
â””â”€ Region-specific (us-east-1, eu-west-1, etc)

OBJECT:
â”œâ”€ Actual file
â”œâ”€ Key: /images/profile-123.jpg
â”œâ”€ Value: file data (bytes)
â”œâ”€ Metadata: size, created date, etc
â””â”€ Can be 0 bytes to 5TB!

STORAGE CLASS:
â”œâ”€ STANDARD: Frequent access (default)
â”œâ”€ INTELLIGENT_TIERING: Auto optimize
â”œâ”€ GLACIER: Archive (cheap, slow access)
â”œâ”€ DEEP_ARCHIVE: Very old files
â””â”€ ONEZONE_IA: Single region, low access

PRICING:
â”œâ”€ Storage: $0.023/GB/month (STANDARD)
â”œâ”€ Transfer out: $0.09/GB (data leaving AWS)
â”œâ”€ Requests: $0.0004 per 1000 PUT/GET
â”œâ”€ Data transfer in: FREE!
â””â”€ Example: 1TB = $23/month
```

---

## ğŸ”¬ Advanced Explanation

### Storage Architecture

**SINGLE FILE STORAGE (Simple):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  S3 Bucket (us-east-1)  â”‚
â”‚                         â”‚
â”‚  â”œâ”€ image-1.jpg         â”‚
â”‚  â”œâ”€ image-2.jpg         â”‚
â”‚  â”œâ”€ video-1.mp4         â”‚
â”‚  â””â”€ document-1.pdf      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Access:
User â†’ S3 direct
Response: 100-500ms (network dependent)

Problems:
âŒ Geographic latency (Australia user waits 500ms)
âŒ Single region (disaster in us-east-1 = all down)
```

**MULTI-REGION WITH CDN (Scalable):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 Bucket    â”‚
â”‚ (us-east-1)  â”‚ â† Origin
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
    replicate
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CloudFront CDN              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ Edge in NY (10ms)                â”‚
â”‚ â”œâ”€ Edge in LA (50ms)                â”‚
â”‚ â”œâ”€ Edge in London (80ms)            â”‚
â”‚ â””â”€ Edge in Sydney (30ms to AU users)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Access:
User â†’ Nearest CDN edge
Response: 50-100ms (global!)

Benefits:
âœ… Fast everywhere
âœ… CDN caches popular files
âœ… If origin down, CDN still serves
âœ… Reduced S3 load
```

### Upload Strategies

**DIRECT UPLOAD (Simple):**

```
Browser â†’ S3 directly

Steps:
1. User selects file
2. Browser sends to S3
3. S3 stores file

Pros:
âœ… Simple
âœ… Server doesn't see file

Cons:
âŒ Browser must authenticate with S3
âŒ Exposes S3 credentials
âŒ No server-side processing
```

**SERVER UPLOAD (Secure):**

```
Browser â†’ Server â†’ S3

Steps:
1. User selects file
2. Browser uploads to YOUR server
3. Server validates, processes, stores in S3
4. Server returns confirmation

Pros:
âœ… Server validates file
âœ… S3 credentials hidden
âœ… Can process before storing
âœ… Rate limiting possible

Cons:
âŒ Server uses bandwidth twice
âŒ Server disk space needed
âŒ More complex
```

**PRESIGNED URL (Best of Both):**

```
Browser â†’ Get presigned URL from server â†’ S3 directly

Steps:
1. Browser: "I want to upload image.jpg"
2. Server: "OK, use this presigned URL (valid 15 min)"
3. Browser: Uploads directly to S3 with URL
4. S3: Validates signature, accepts upload
5. Server gets webhook notification

Pros:
âœ… Direct upload (fast, no server bandwidth)
âœ… Secure (signature-based)
âœ… Server controls who can upload
âœ… Server can validate metadata

Used by: Most apps (Dropbox, Google Photos, etc)
```

### Data Organization

**FLAT STRUCTURE (Simple):**

```
Bucket:
â”œâ”€ image-1.jpg
â”œâ”€ image-2.jpg
â”œâ”€ image-3.jpg
â”œâ”€ video-1.mp4
â”œâ”€ document-1.pdf

Problem:
âŒ Hard to organize
âŒ No logical grouping
âŒ Can't apply policies easily
```

**HIERARCHICAL STRUCTURE (Organized):**

```
Bucket:
â”œâ”€ users/
â”‚  â”œâ”€ alice/
â”‚  â”‚  â”œâ”€ profile.jpg
â”‚  â”‚  â”œâ”€ header.png
â”‚  â”‚  â””â”€ settings.json
â”‚  â””â”€ bob/
â”‚     â”œâ”€ profile.jpg
â”‚     â””â”€ avatar.png
â”œâ”€ uploads/
â”‚  â”œâ”€ 2024/01/
â”‚  â”‚  â”œâ”€ file1.pdf
â”‚  â”‚  â””â”€ file2.docx
â”‚  â””â”€ 2024/02/
â”‚     â””â”€ file3.xlsx
â””â”€ archives/
   â”œâ”€ 2023-backup.tar.gz
   â””â”€ 2022-backup.tar.gz

Benefits:
âœ… Organized
âœ… Easy to apply policies
âœ… Can expire old files
âœ… Natural grouping
```

### Lifecycle Policies

```
Automatically move/delete files based on age

POLICY EXAMPLE:

â”œâ”€ STANDARD: Days 0-30 (hot data)
â”‚  Access frequently, keep fast
â”‚
â”œâ”€ INTELLIGENT_TIERING: Days 31-90 (warming)
â”‚  Accessed occasionally
â”‚  Auto-optimize cost
â”‚
â”œâ”€ GLACIER: Days 91-365 (cold data)
â”‚  Rarely accessed
â”‚  Cheap, slow retrieval (hours)
â”‚
â””â”€ DELETE: After 365 days (archive/delete)
   Old backup files gone

Cost reduction:
100GB on STANDARD: $2.30/month
100GB moved to GLACIER: $0.40/month
Savings: 83%! ğŸ’°
```

### Permissions & Security

```
BUCKET POLICIES:

1. Public Read (anyone can GET):
   âœ… Images for website
   âœ… Public documents
   âœ… Downloadable assets

2. Private (only authenticated):
   âœ… User files
   âœ… Private documents
   âœ… Sensitive data

3. Restricted (specific users):
   âœ… Team files
   âœ… Internal documents
   âœ… Shared projects

ENCRYPTION:

1. In Transit (HTTPS):
   âœ… File encrypted while uploading

2. At Rest (Server-side):
   âœ… File encrypted while stored
   âœ… S3 manages keys (default)
   âœ… Or you manage keys (KMS)

3. Client-side:
   âœ… You encrypt before upload
   âœ… Only you can decrypt
```

---

## ğŸ Python Code Example

### âŒ Without File Storage (Database Bloat)

```python
# ===== WITHOUT FILE STORAGE (BAD) =====

import sqlite3

class BadFileDB:
    """Stores files directly in database (bad idea!)"""
    
    def __init__(self):
        self.conn = sqlite3.connect(':memory:')
        self.cursor = self.conn.cursor()
        
        # Create table with BLOB column (ğŸš© red flag!)
        self.cursor.execute('''
            CREATE TABLE user_files (
                id INTEGER PRIMARY KEY,
                user_id INTEGER,
                filename TEXT,
                file_data BLOB
            )
        ''')
        self.conn.commit()
    
    def upload_file(self, user_id, filename, file_data):
        """Upload file to database (bloats database!)"""
        self.cursor.execute(
            '''INSERT INTO user_files (user_id, filename, file_data)
               VALUES (?, ?, ?)''',
            (user_id, filename, file_data)
        )
        self.conn.commit()
    
    def get_file(self, file_id):
        """Retrieve file from database"""
        self.cursor.execute(
            'SELECT filename, file_data FROM user_files WHERE id = ?',
            (file_id,)
        )
        return self.cursor.fetchone()

# Problems:
# âŒ Database grows HUGE (10GB file = 10GB database!)
# âŒ Backups massive
# âŒ Queries slow (file data mixed with metadata)
# âŒ Can't expire old files efficiently
# âŒ No built-in redundancy
# âŒ Expensive!

# Example:
db = BadFileDB()
large_file = b"x" * (100 * 1024 * 1024)  # 100MB file
db.upload_file(1, "large.bin", large_file)
print("Stored 100MB in database... ğŸ’€")
```

### âœ… With File Storage (S3)

```python
# ===== WITH FILE STORAGE (S3) =====

import boto3
from datetime import datetime

class S3FileStorage:
    """Store files in S3, metadata in database"""
    
    def __init__(self, bucket_name):
        self.s3 = boto3.client('s3')
        self.bucket = bucket_name
        self.metadata = {}  # Simulate database
        self.file_id = 1
    
    def upload_file(self, user_id, filename, file_data):
        """Upload file to S3, store metadata"""
        
        # Generate unique key
        key = f"users/{user_id}/{filename}"
        
        # Upload to S3
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=file_data,
            Metadata={
                'user_id': str(user_id),
                'uploaded_at': datetime.now().isoformat()
            }
        )
        
        # Store METADATA in database (tiny!)
        self.metadata[self.file_id] = {
            'user_id': user_id,
            'filename': filename,
            's3_key': key,
            'size_bytes': len(file_data),
            'uploaded_at': datetime.now().isoformat()
        }
        
        file_id = self.file_id
        self.file_id += 1
        
        return file_id
    
    def get_file_url(self, file_id, expiration_seconds=3600):
        """Get pre-signed URL for file access"""
        
        if file_id not in self.metadata:
            return None
        
        key = self.metadata[file_id]['s3_key']
        
        # Generate presigned URL (valid for 1 hour)
        url = self.s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': key},
            ExpiresIn=expiration_seconds
        )
        
        return url
    
    def delete_file(self, file_id):
        """Delete file from S3"""
        
        if file_id not in self.metadata:
            return False
        
        key = self.metadata[file_id]['s3_key']
        
        self.s3.delete_object(Bucket=self.bucket, Key=key)
        del self.metadata[file_id]
        
        return True
    
    def get_storage_stats(self):
        """Get storage statistics"""
        total_size = sum(f['size_bytes'] for f in self.metadata.values())
        return {
            'files': len(self.metadata),
            'total_size_gb': total_size / (1024 ** 3),
            'cost_per_month': (total_size / (1024 ** 3)) * 0.023
        }

# Benefits:
# âœ… Database stays small (only metadata)
# âœ… Files stored in unlimited S3
# âœ… Automatic backups
# âœ… Cheap!
# âœ… Global access
# âœ… No server storage needed

# Example usage (requires AWS credentials):
# storage = S3FileStorage('my-bucket')
# large_file = b"x" * (100 * 1024 * 1024)  # 100MB
# file_id = storage.upload_file(1, "large.bin", large_file)
# url = storage.get_file_url(file_id)
# print(f"Download at: {url}")
```

### âœ… Production File Storage (Advanced)

```python
# ===== PRODUCTION FILE STORAGE =====

import boto3
from botocore.exceptions import ClientError
import hashlib
from datetime import datetime, timedelta

class ProductionFileStorage:
    """Production-grade file storage with best practices"""
    
    def __init__(self, bucket_name, region='us-east-1'):
        self.s3 = boto3.client('s3', region_name=region)
        self.bucket = bucket_name
        self.region = region
    
    def upload_file_with_validation(self, user_id, filename, file_data, 
                                    max_size_mb=100, allowed_types=None):
        """Upload with validation"""
        
        # Validate file size
        file_size_mb = len(file_data) / (1024 ** 2)
        if file_size_mb > max_size_mb:
            raise ValueError(f"File too large: {file_size_mb}MB > {max_size_mb}MB")
        
        # Validate file type (by extension)
        if allowed_types:
            ext = filename.split('.')[-1].lower()
            if ext not in allowed_types:
                raise ValueError(f"File type {ext} not allowed")
        
        # Generate unique key
        file_hash = hashlib.md5(file_data).hexdigest()
        date_path = datetime.now().strftime('%Y/%m/%d')
        key = f"users/{user_id}/{date_path}/{file_hash}-{filename}"
        
        try:
            # Upload with encryption & metadata
            self.s3.put_object(
                Bucket=self.bucket,
                Key=key,
                Body=file_data,
                ServerSideEncryption='AES256',
                Metadata={
                    'user_id': str(user_id),
                    'original_filename': filename,
                    'uploaded_at': datetime.now().isoformat()
                },
                ContentType=self._get_content_type(filename),
                StorageClass='INTELLIGENT_TIERING'  # Auto-optimize costs
            )
            
            return {
                'success': True,
                's3_key': key,
                'size_bytes': len(file_data),
                'uploaded_at': datetime.now().isoformat()
            }
        
        except ClientError as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_presigned_url(self, s3_key, expiration_hours=1):
        """Get presigned URL for secure download"""
        
        try:
            url = self.s3.generate_presigned_url(
                'get_object',
                Params={'Bucket': self.bucket, 'Key': s3_key},
                ExpiresIn=expiration_hours * 3600
            )
            return url
        except ClientError as e:
            return None
    
    def get_presigned_post_url(self, user_id, filename, max_size_mb=100):
        """Get presigned POST URL for browser upload"""
        
        key = f"users/{user_id}/{filename}"
        
        try:
            response = self.s3.generate_presigned_post(
                Bucket=self.bucket,
                Key=key,
                ExpiresIn=3600,
                Conditions=[
                    ['content-length-range', 0, max_size_mb * 1024 * 1024]
                ]
            )
            return response
        except ClientError as e:
            return None
    
    def setup_lifecycle_policy(self):
        """Set up automatic archival for old files"""
        
        lifecycle_config = {
            'Rules': [
                {
                    'Id': 'Archive old files',
                    'Status': 'Enabled',
                    'Prefix': 'users/',
                    'Transitions': [
                        {
                            'Days': 30,
                            'StorageClass': 'GLACIER'  # Move to cold storage
                        },
                        {
                            'Days': 365,
                            'StorageClass': 'DEEP_ARCHIVE'  # Move to archive
                        }
                    ],
                    'Expiration': {
                        'Days': 2555  # Delete after 7 years
                    }
                }
            ]
        }
        
        try:
            self.s3.put_bucket_lifecycle_configuration(
                Bucket=self.bucket,
                LifecycleConfiguration=lifecycle_config
            )
            return True
        except ClientError:
            return False
    
    def _get_content_type(self, filename):
        """Determine content type from filename"""
        ext_to_type = {
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'png': 'image/png',
            'gif': 'image/gif',
            'pdf': 'application/pdf',
            'mp4': 'video/mp4',
            'txt': 'text/plain'
        }
        ext = filename.split('.')[-1].lower()
        return ext_to_type.get(ext, 'application/octet-stream')

# Usage
# storage = ProductionFileStorage('my-bucket')
# 
# # Upload with validation
# result = storage.upload_file_with_validation(
#     user_id=1,
#     filename='profile.jpg',
#     file_data=open('profile.jpg', 'rb').read(),
#     max_size_mb=10,
#     allowed_types=['jpg', 'jpeg', 'png']
# )
# 
# # Get download URL
# url = storage.get_presigned_url(result['s3_key'])
# 
# # Get browser upload URL
# post_data = storage.get_presigned_post_url(1, 'avatar.jpg')
# 
# # Set up auto-archival
# storage.setup_lifecycle_policy()
```

---

## ğŸ’¡ Mini Project: "Build a File Management System"

### Phase 1: Simple Upload/Download â­

**Requirements:**
- Upload files to S3
- List files
- Download files
- Delete files
- Basic metadata

---

### Phase 2: Advanced (With Validation) â­â­

**Requirements:**
- File type validation
- Size limits
- Pre-signed URLs
- Lifecycle policies
- Cost tracking

---

### Phase 3: Enterprise (Full System) â­â­â­

**Requirements:**
- Multi-tenant support
- Access control
- Virus scanning
- CDN integration
- Analytics dashboard

---

## âš–ï¸ Storage Services Comparison

| Feature | AWS S3 | Azure Blob | Google Cloud | MinIO |
|---------|--------|-----------|--------------|-------|
| **Cost** | $0.023/GB | $0.018/GB | $0.020/GB | Self-hosted |
| **Regions** | 30+ | 60+ | 40+ | Any |
| **Durability** | 11 nines | 11 nines | 11 nines | Configurable |
| **Throughput** | Unlimited | High | High | Limited |
| **CDN Integration** | CloudFront | Built-in | Built-in | External |
| **Ease of Use** | Very easy | Easy | Easy | Hard |
| **Enterprise** | âœ… | âœ… | âœ… | âŒ |

---

## ğŸ¯ When to Use File Storage

```
âœ… USE FILE STORAGE WHEN:
- Large files (>10MB)
- Images, videos, documents
- Millions of files
- Global distribution needed
- Cost matters
- Unlimited capacity needed

âŒ DON'T USE WHEN:
- Small metadata only
- Relational queries needed
- Strong consistency required
- Need random access (HDD better)
```

---

## âŒ Common Mistakes

### Mistake 1: Storing Everything in Database

```python
# âŒ Don't do this
database.store_image_blob(image_file)

# âœ… Do this
s3.upload(image_file)
database.store_metadata(filename, s3_key)
```

### Mistake 2: Not Setting Expiration

```python
# âŒ Old files stay forever
# âœ… Set lifecycle policy
storage.setup_lifecycle_policy()
# Auto-delete after 7 years
```

### Mistake 3: Public Access

```python
# âŒ Anyone can download
bucket.make_public()

# âœ… Use presigned URLs
url = storage.get_presigned_url(key, expiration_hours=1)
# Valid for 1 hour only
```

---

## ğŸ“š Additional Resources

**AWS S3:**
- [S3 Documentation](https://docs.aws.amazon.com/s3/)
- [S3 Pricing](https://aws.amazon.com/s3/pricing/)
- [S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/BestPractices.html)

**Alternatives:**
- [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs/)
- [Google Cloud Storage](https://cloud.google.com/storage)
- [MinIO](https://min.io/) - Open source S3-compatible


---

## ğŸ¯ Before You Leave

**Can you answer these?**

1. **Why not store files in a database?**
   - Answer: Bloats database, slow backups, expensive

2. **What's a presigned URL?**
   - Answer: Time-limited URL for secure access without credentials

3. **What's S3 storage class?**
   - Answer: Different tiers (STANDARD, GLACIER, etc) for different access patterns

4. **How do you organize S3 files?**
   - Answer: Hierarchical paths (users/alice/profile.jpg)

5. **What's lifecycle policy?**
   - Answer: Auto-move/delete files based on age to reduce costs

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **Developer:** "I stored all our files in S3!"
>
> **CEO:** "Great! How much does it cost?"
>
> **Developer:** "Well... we have a 10TB bucket..."
>
> **CEO:** "And?"
>
> **Developer:** "...with no lifecycle policy. It's been 5 years."
>
> **CEO:** *faints* ğŸ’€

---

[â† Back to Main](../README.md) | [Previous: Data Modeling](15-data-modeling.md) | [Next: Load Balancers â†’](17-load-balancers.md)

---

**Last Updated:** November 10, 2025  
**Difficulty:** â­â­ Beginner-Intermediate (cloud concepts)  
**Time to Read:** 24 minutes  
**Time to Build System:** 3-5 hours per phase  

---

*File storage: Where your files go to live their best cloud life, forever and ever, costing you $23/month.* ğŸš€