# 11. CAP Theorem

CAP Theorem says you can only have 2 of 3 guarantees. Developers read this and think "I'll take all three!" Then their system crashes at 2 AM and they understand. ğŸ’”

[â† Back to Main](../README.md) | [Previous: Replication](10-replication.md) | [ACID vs BASE](12-acid-vs-base.md)

---

## ğŸ¯ Quick Summary

**The CAP Theorem** states that in a distributed system, you can guarantee only two of three properties: **Consistency** (all nodes see same data), **Availability** (system always responds), or **Partition tolerance** (survives network splits). Every real distributed system picks two and accepts the trade-off of the third.

Think of it as: **You Get 2 of 3: Consistency, Availability, or Partition Tolerance**

---

## ğŸŒŸ Beginner Explanation

### The Three Properties

**CONSISTENCY (C):**
```
All nodes have the same data at the same time.

Bank Account Balance:
- Account has $100
- Master says: $100
- Replica 1 says: $100
- Replica 2 says: $100

Write Update: +$50
- Master: $150
- Replica 1: $150
- Replica 2: $150

All consistent âœ…

If you read balance from ANY server: Always $150
```

**AVAILABILITY (A):**
```
System always responds to requests (never returns "Error").

Request 1: Get balance â†’ Server responds (200ms)
Request 2: Transfer $50 â†’ Server responds (150ms)
Request 3: Get balance â†’ Server responds (180ms)

System handles all requests âœ…

Even if slow, as long as it responds: Available âœ…
```

**Partition Tolerance (P):**
```
System survives network partitions (servers can't talk to each other).

Scenario: Network cable cut! ğŸ”Œ

Network Partition:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Server 1   â”‚ DISCONNECTED â”‚  Server 2   â”‚
â”‚  Master     â”‚ CAN'T TALK   â”‚  Replica    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

System must keep running despite this ğŸ’ª

Partition Tolerant: Yes, both keep accepting requests
Partition Tolerant: No, one shuts down
```

### The Three Trade-offs

**OPTION 1: CP (Consistency + Partition Tolerance)**

```
During network partition:
â”œâ”€ Master: Accepts writes, consistent âœ…
â”œâ”€ Replica: Disconnected, rejects reads/writes âŒ
â””â”€ Result: System available on master only

Consistency: Guaranteed âœ…
Availability: Lost (replicas down) âŒ
Partition Tolerance: Handled âœ…

Example: Traditional SQL databases
â”œâ”€ Primary: Takes writes âœ…
â”œâ”€ Standby: Can't write (might be inconsistent)
â”œâ”€ After reconnect: Replicas catch up âœ…

When to use: Banking, critical data
```

**OPTION 2: AP (Availability + Partition Tolerance)**

```
During network partition:
â”œâ”€ Server 1: Accepts all writes, returns results âœ…
â”œâ”€ Server 2: Accepts all writes, returns results âœ…
â”œâ”€ After reconnect: Servers have DIFFERENT data ğŸ˜±
â””â”€ Conflict resolution needed

Consistency: Lost (eventually consistent) âŒ
Availability: Guaranteed âœ…
Partition Tolerance: Handled âœ…

Example: NoSQL databases (MongoDB, Cassandra)
â”œâ”€ Write to Server 1: Succeeds immediately âœ…
â”œâ”€ Write to Server 2: Succeeds immediately âœ…
â”œâ”€ Servers reconnect: Merge/resolve conflicts
â””â”€ Eventually consistent after merge

When to use: Social media, analytics
```

**OPTION 3: CA (Consistency + Availability)**

```
Assumes no network partitions (unrealistic!)

If any partition happens:
â”œâ”€ You must choose: Keep CA or handle partition?
â”œâ”€ You MUST pick either CP or AP
â””â”€ CA is not possible in distributed systems!

Why? When networks partition:
- Can't have consistency (nodes diverge)
- Can't have availability (someone must go down)

Example: Single server (not distributed)
â”œâ”€ No partitions possible âœ…
â”œâ”€ One server = always consistent âœ…
â”œâ”€ One server = always available âœ…

But if server crashes: Neither âŒ
```

### Visual: The CAP Triangle

```
        Consistency
            /\
           /  \
          /    \
         /  CA  \
        /________\
       /         /\
      /    CP   /  \
     /         /    \
    /__________/      \
   /    AP    /________\
  /          /          \
 /__________/____________\

   Partition Tolerance

Every system sits somewhere on this triangle.
Must give up at least one corner.
```

---

## ğŸ”¬ Advanced Explanation

### Consistency Models

**STRONG CONSISTENCY (Linear Consistency)**

```
Write completes â†’ All reads return new value

Timeline:
Write: Set age = 30 (completes at T=100ms)
Read at T=101ms: Gets 30 âœ…
Read at T=102ms: Gets 30 âœ…
Read at T=200ms: Gets 30 âœ…

Guarantee: All reads after write see new value

Cost: Slow (must replicate everywhere before confirming)
```

**EVENTUAL CONSISTENCY**

```
Write completes â†’ Reads eventually return new value (after lag)

Timeline:
Write: Set age = 30 (completes at T=100ms locally)
Read at T=101ms: Gets old value (30 not replicated yet) ğŸ˜±
Read at T=200ms: Gets 30 âœ…
Read at T=500ms: Gets 30 âœ…

Guarantee: All reads eventually see new value (after lag)

Cost: Fast (confirm write immediately, replicate later)
Problem: Temporary inconsistency
```

**CAUSAL CONSISTENCY**

```
Respects cause-and-effect between operations

Operation 1: Write X = 10
Operation 2: Write Y = X + 5 (depends on Op 1)
Operation 3: Read Y

With causal consistency:
- Op 3 sees Y = 15 (knows about Op 1 and 2)
- Not arbitrary ordering

Middle ground between:
- Strong consistency (too slow)
- Eventual consistency (confusing)
```

### Real-World CAP Choices

**BANKING (CP: Consistency + Partition Tolerance)**

```
Network partition: Choose consistency

Scenario: User tries to withdraw $100

Path 1 (Has connection to master):
â”œâ”€ Master: Check balance ($150)
â”œâ”€ Master: Withdraw $100
â”œâ”€ Result: Success âœ…, balance = $50

Path 2 (Disconnected from master):
â”œâ”€ Replica can't write (no master connection)
â”œâ”€ Result: "Error, try again" âŒ
â”œâ”€ Reason: Better to lose availability than lose consistency
â”œâ”€ (Don't want to allow two $100 withdrawals from $150!)

Better consistency than availability ğŸ’ª
```

**SOCIAL MEDIA (AP: Availability + Partition Tolerance)**

```
Network partition: Choose availability

Scenario: User posts tweet

Path 1 (Server A):
â”œâ”€ Server A: Accept tweet immediately âœ…
â”œâ”€ Server A: "Posted!" to user
â”œâ”€ Later: Replicate to Server B, C, D

Path 2 (Server B disconnected):
â”œâ”€ Still accepts posts immediately âœ…
â”œâ”€ User gets response instantly
â”œâ”€ Later: Merge/sync when reconnected

Better availability than consistency ğŸ’ª
```

### PACELC Theorem (Extension of CAP)

```
CAP says: During Partition, choose Consistency or Availability

But what about normal operation (no partition)?

PACELC adds:
- If Partition: Choose C or A (CAP)
- Else (no partition): Choose Latency or Consistency

PACELC says:
Every system trades Partitions (AC vs AP)
AND trades Latency (E) vs Consistency (C)

Examples:

PostgreSQL (PC/EC):
â”œâ”€ Partition: Choose Consistency
â”œâ”€ Normal: Consistency over Latency (slow writes)

Cassandra (PA/EL):
â”œâ”€ Partition: Choose Availability
â”œâ”€ Normal: Latency over Consistency (fast writes, eventual consistency)

Riak (PA/EL):
â”œâ”€ Partition: Choose Availability
â”œâ”€ Normal: Latency over Consistency
```

### Handling Network Partitions

**STRATEGY 1: Stop One Side (CP)**

```
Network cuts! ğŸ”Œ

[Server 1]              [Server 2]
Master              Replica
Can't talk!

Strategy: Fence off one side
â”œâ”€ Master keeps running: Accepts writes âœ…
â”œâ”€ Replica shuts down: Rejects requests âŒ
â”œâ”€ Result: Consistent but not available on replica
â”œâ”€ After reconnect: Replica catches up from master

Example: Traditional database failover
```

**STRATEGY 2: Keep Both Running (AP)**

```
Network cuts! ğŸ”Œ

[Server 1]              [Server 2]
Accepts writes          Accepts writes
Both keep running âœ…    Both keep running âœ…

Results:
- Write to S1: age = 30
- Write to S2: age = 25

After reconnect:
- Age = 30 or 25? âŒ
- Need conflict resolution:
  - Last-write-wins: Keep later write
  - Application logic: Merge intelligently
  - User decides: Ask user which version to keep

Example: NoSQL with multi-master
```

### Practical Decisions During Partitions

```
PARTITION HAPPENS: You have seconds to decide!

Decision Tree:

Q1: Is this data critical?
â”œâ”€ YES: Choose CP (lose availability)
â”‚   â””â”€ Banking, healthcare: Can't afford inconsistency
â”‚
â””â”€ NO: Choose AP (lose consistency)
    â””â”€ Social media, analytics: Can tolerate stale data

Q2: Can you afford downtime?
â”œâ”€ NO: Choose AP (keep serving)
â”‚   â””â”€ Always-on services
â”‚
â””â”€ YES: Choose CP (consistent or fail)
    â””â”€ Internal tools, less critical systems

Q3: How bad is inconsistency?
â”œâ”€ CATASTROPHIC: Choose CP
â”‚   â””â”€ Financial, medical
â”‚
â””â”€ ACCEPTABLE: Choose AP
    â””â”€ Preferences, feed ranking
```

---

## ğŸ Python Code Example

### âŒ Naive Distributed System (Misses CAP)

```python
# ===== NAIVE SYSTEM (CRASHES ON PARTITION) =====

class NaiveDistributedDB:
    """System that doesn't handle partitions"""
    
    def __init__(self):
        self.master_data = {}
        self.replica_data = {}
        self.network_connected = True
    
    def write(self, key, value):
        """Write to master"""
        # Write to master
        self.master_data[key] = value
        
        # Replicate to replica
        if self.network_connected:
            self.replica_data[key] = value
            return True
        else:
            # Network down! What do we do?
            # Naive: Just ignore
            # Result: Inconsistency ğŸ˜±
            return True  # Lie to user
    
    def read_from_replica(self, key):
        """Read from replica"""
        # Returns stale data if write failed to replicate!
        return self.replica_data.get(key)
    
    def simulate_network_partition(self):
        """Simulate network cut"""
        self.network_connected = False

# Problems:
# âŒ Doesn't handle network partitions
# âŒ Can lose data (writes confirmed but not replicated)
# âŒ No consistency guarantee
# âŒ No availability guarantee
```

### âœ… CP System (Consistent, Partition Tolerant)

```python
from enum import Enum

class ConsistencyChoice(Enum):
    STOP = "stop"        # Reject requests
    ASYNC_CACHE = "async"  # Cache locally, sync later

class CPDistributedDB:
    """Prioritizes Consistency over Availability"""
    
    def __init__(self, choice=ConsistencyChoice.STOP):
        self.master_data = {}
        self.replica_data = {}
        self.network_connected = True
        self.consistency_choice = choice
        self.pending_writes = []  # For async mode
    
    def write(self, key, value):
        """Write with consistency guarantee"""
        
        if not self.network_connected:
            # Network partition happened!
            if self.consistency_choice == ConsistencyChoice.STOP:
                # Stop: Return error (lose availability)
                return {"success": False, "error": "Network down, rejecting write"}
            elif self.consistency_choice == ConsistencyChoice.ASYNC_CACHE:
                # Cache: Store locally, sync later
                self.pending_writes.append((key, value))
                return {"success": True, "pending": True}
        
        # Normal path: Replicate to replica before confirming
        self.master_data[key] = value
        self.replica_data[key] = value  # Wait for this!
        
        return {"success": True, "pending": False}
    
    def read(self, key):
        """Read is always consistent"""
        return self.master_data.get(key)
    
    def simulate_network_partition(self):
        """Partition happens"""
        self.network_connected = False
    
    def recover_from_partition(self):
        """Network comes back"""
        self.network_connected = True
        
        # Flush pending writes
        for key, value in self.pending_writes:
            self.replica_data[key] = value
        
        self.pending_writes = []
    
    def get_status(self):
        return {
            "master_data": self.master_data,
            "replica_data": self.replica_data,
            "network_connected": self.network_connected,
            "pending_writes": len(self.pending_writes)
        }

# Usage
print("=== CP SYSTEM (Consistency + Partition Tolerance) ===\n")

db = CPDistributedDB(choice=ConsistencyChoice.STOP)

# Normal operation
print("Write: alice_age = 30")
result = db.write("alice_age", 30)
print(f"Result: {result}")

print(f"Read: {db.read('alice_age')}")

# Network partition!
print("\nğŸ”Œ Network partition!")
db.simulate_network_partition()

print("Attempt write: alice_age = 35")
result = db.write("alice_age", 35)
print(f"Result: {result}")
# Returns error! âŒ Lose availability but keep consistency âœ…

print("\nâœ… Network recovered!")
db.recover_from_partition()
print(f"Status: {db.get_status()}")

# Output:
# === CP SYSTEM ===
#
# Write: alice_age = 30
# Result: {'success': True, 'pending': False}
# Read: 30
#
# ğŸ”Œ Network partition!
# Attempt write: alice_age = 35
# Result: {'success': False, 'error': 'Network down, rejecting write'}
#
# âœ… Network recovered!
# Status: {'master_data': {'alice_age': 30}, 'replica_data': {'alice_age': 30}, ...}
```

### âœ… AP System (Available, Partition Tolerant)

```python
class APDistributedDB:
    """Prioritizes Availability over Consistency"""
    
    def __init__(self):
        self.master_data = {}
        self.replica_data = {}
        self.network_connected = True
        self.master_version = {}
        self.replica_version = {}
        self.version_counter = 0
    
    def write(self, key, value):
        """Write with immediate confirmation (eventually consistent)"""
        
        self.version_counter += 1
        version = self.version_counter
        
        # Always accept write (availability!)
        self.master_data[key] = value
        self.master_version[key] = version
        
        # Try to replicate
        if self.network_connected:
            self.replica_data[key] = value
            self.replica_version[key] = version
        # If network down: Still confirm write! (Accept eventual inconsistency)
        
        return {"success": True, "version": version}
    
    def read(self, key):
        """Read from either node"""
        master_val = self.master_data.get(key)
        master_ver = self.master_version.get(key, 0)
        
        replica_val = self.replica_data.get(key)
        replica_ver = self.replica_version.get(key, 0)
        
        # Return newer version
        if master_ver >= replica_ver:
            return master_val
        else:
            return replica_val
    
    def simulate_network_partition(self):
        """Partition happens"""
        self.network_connected = False
    
    def recover_from_partition(self):
        """Network comes back, resolve conflicts"""
        self.network_connected = True
        
        # Simple conflict resolution: Last-write-wins
        for key in self.master_data:
            master_ver = self.master_version[key]
            replica_ver = self.replica_version.get(key, 0)
            
            if master_ver > replica_ver:
                # Master is newer, replica catches up
                self.replica_data[key] = self.master_data[key]
                self.replica_version[key] = master_ver
        
        for key in self.replica_data:
            replica_ver = self.replica_version[key]
            master_ver = self.master_version.get(key, 0)
            
            if replica_ver > master_ver:
                # Replica is newer, master catches up
                self.master_data[key] = self.replica_data[key]
                self.master_version[key] = replica_ver

# Usage
print("=== AP SYSTEM (Availability + Partition Tolerance) ===\n")

db = APDistributedDB()

# Normal operation
print("Write: alice_age = 30")
result = db.write("alice_age", 30)
print(f"Result: {result}")

# Network partition!
print("\nğŸ”Œ Network partition!")
db.simulate_network_partition()

print("Write: alice_age = 35")
result = db.write("alice_age", 35)
print(f"Result: {result}")
print("âœ… Write accepted despite partition! (Availability)")

# Different write on replica (simulated)
db.replica_data["alice_age"] = 25
db.replica_version["alice_age"] = 2

print(f"\nMaster has: {db.master_data['alice_age']}")
print(f"Replica has: {db.replica_data['alice_age']}")
print("âŒ Inconsistent during partition")

print("\nâœ… Network recovered!")
db.recover_from_partition()

print(f"After conflict resolution:")
print(f"Master has: {db.master_data['alice_age']}")
print(f"Replica has: {db.replica_data['alice_age']}")
print("âœ… Consistent again (eventually)")

# Output:
# === AP SYSTEM ===
# Write: alice_age = 30
# Result: {'success': True, 'version': 1}
#
# ğŸ”Œ Network partition!
# Write: alice_age = 35
# Result: {'success': True, 'version': 2}
# âœ… Write accepted despite partition!
#
# Master has: 35
# Replica has: 25
# âŒ Inconsistent during partition
#
# âœ… Network recovered!
# After conflict resolution:
# Master has: 35
# Replica has: 35
# âœ… Consistent again (eventually)
```

---

## ğŸ’¡ Mini Project: "CAP Trade-off Explorer"

### Phase 1: Simple Systems â­

**Requirements:**
- Simulate CP system
- Simulate AP system
- Show differences
- Compare trade-offs

---

### Phase 2: Advanced (With Conflicts) â­â­

**Requirements:**
- Multiple concurrent writes
- Conflict detection
- Resolution strategies
- Consistency tracking

---

### Phase 3: Enterprise (Real Scenarios) â­â­â­

**Requirements:**
- Network partition simulation
- Multiple regions
- Automatic failover
- Conflict resolution
- Monitoring

---

## âš–ï¸ CAP Choices by System

| System | Choice | Why |
|--------|--------|-----|
| **PostgreSQL** | CP | Critical data, consistency matters |
| **MongoDB** | AP | High availability, eventual consistency |
| **Cassandra** | AP | Distributed, high availability |
| **Dynamo (AWS)** | AP | Always available, tolerates stale data |
| **Chubby (Google)** | CP | Coordination, need consistency |
| **Redis** | CP | In-memory, can afford to go down |
| **HBase** | CP | Strong consistency for writes |
| **Memcached** | AP | Lossy cache, availability > consistency |

---

## âŒ Common Misunderstandings

### Mistake 1: "I'll Just Have All Three"

```python
# âŒ NO! Impossible in distributed systems
# If network partition happens, you MUST choose

# What happens if you try to have all three:
â”œâ”€ Network partition occurs
â”œâ”€ Master and replica can't talk
â”œâ”€ Master allows write (availability)
â”œâ”€ Replica allows write (availability)
â”œâ”€ Different data on each (inconsistency)
â”œâ”€ Broke consistency! ğŸ’¥
```

### Mistake 2: "Partitions Never Happen"

```python
# âŒ WRONG! Partitions absolutely happen
# Real-world examples:
â”œâ”€ Fiber cut ğŸ”ª
â”œâ”€ Misconfigured firewall ğŸ”¥
â”œâ”€ Router failure ğŸ’”
â”œâ”€ BGP hijack ğŸ˜±
â”œâ”€ Cloud provider issues â›ˆï¸

# Partitions are guaranteed to happen
# You must plan for them

# âœ… Every distributed system MUST be partition tolerant
# So you're really choosing between CP and AP
```

### Mistake 3: Not Understanding Trade-offs

```python
# âŒ Choosing AP thinking consistency doesn't matter
# Result: Corrupted data, inconsistent state, angry users

# When building system:
# 1. Identify what data you have
# 2. Understand consistency requirements
# 3. THEN choose CAP strategy

# âŒ Financial data? Must be consistent (CP)
# âŒ Medical data? Must be consistent (CP)
# âœ… Social media feed? Can be eventually consistent (AP)
```

---

## ğŸ“š Additional Resources

**Foundational Papers:**
- [CAP Theorem - Eric Brewer](https://www.infoq.com/presentation/cap-theorem-2015/)
- [PACELC - Daniel Abadi](http://dbmsmusings.blogspot.com/2010/04/problems-with-cap-theorem.html)

**Real-world Examples:**
- [Consistency & Trade-offs in Distributed Systems](https://www.youtube.com/watch?v=cy9THvANvv4)
- [CRDTs & Eventual Consistency](https://crdt.tech/)

**Tools & Systems:**
- [Jepsen - Partition Testing](https://jepsen.io/)
- [Hermitage - Database Testing](https://github.com/ept/hermitage)

---

## ğŸ¯ Before You Leave

**Can you answer these?**

1. **What are the three properties in CAP?**
   - Answer: Consistency, Availability, Partition tolerance

2. **Can a system have all three?**
   - Answer: No. With partition, must choose C or A

3. **Why do network partitions happen?**
   - Answer: Cables break, routers fail, software bugs, etc.

4. **What does CP prioritize?**
   - Answer: Consistency over availability (fails closed)

5. **What does AP prioritize?**
   - Answer: Availability over consistency (fails open)

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **New Developer:** "I'll build a system with Consistency, Availability, AND Partition Tolerance!"
>
> **Senior Dev:** "Cool. You'll learn about CAP Theorem the hard way."
>
> **3 Months Later:** Network partition happens
>
> **New Developer:** *stares in horror as data becomes inconsistent*
>
> **Senior Dev:** "Welcome to distributed systems." ğŸ˜

---

[â† Back to Main](../README.md) | [Previous: Replication](10-replication.md) | [ACID vs BASE](12-acid-vs-base.md)

---

**Last Updated:** November 10, 2025  
**Difficulty:** â­â­â­ Intermediate-Advanced (requires distributed thinking)  
**Time to Read:** 23 minutes  
**Time to Build Explorer:** 3-5 hours per phase  

---

*CAP Theorem: The most important thing to understand when everything goes wrong.* ğŸš€