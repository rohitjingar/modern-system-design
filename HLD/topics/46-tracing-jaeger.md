# 46. Distributed Tracing (Jaeger, OpenTelemetry)

A user clicks a button. 50 services get involved. Request goes through: API gateway, auth service, database, cache, message queue, payment processor, analytics, logging, monitoring... One service is slow. Which one? You don't know. You have logs from all 50. You have metrics from all 50. But no trace showing which one actually broke the chain. Distributed tracing: finally understanding what your microservices are doing! ğŸ”ğŸ›

[â† Back to Main](../README.md) | [Previous: Metrics](45-metrics-prometheus.md) | [Next: URL Shortener](../case-studies/47-url-shortener.md)

---

## ğŸ¯ Quick Summary

**Distributed Tracing** tracks requests across multiple services. **Trace** follows one request through system (spans = service calls). **Jaeger** collects and visualizes traces. **OpenTelemetry** standard for instrumentation. Uber created Jaeger. Traces show: latency bottlenecks, service dependencies, failure paths. Trade-off: overhead (adds latency), storage (massive), sampling (miss rare issues). Essential for microservices debugging.

Think of it as: **Distributed Tracing = Request Journey Map**

---

## ğŸŒŸ Beginner Explanation

### Problem: Debugging Microservices

```
SCENARIO: User reports "checkout is slow"

Monolith (Old):
â”œâ”€ GET /checkout
â”œâ”€ Server processes
â”œâ”€ Returns response
â””â”€ Look at logs, see problem

Microservices (Now):
â”œâ”€ GET /checkout â†’ API Gateway
â”œâ”€ â†’ Auth Service (slow?)
â”œâ”€ â†’ Order Service (slow?)
â”œâ”€ â†’ Payment Service (slow?)
â”œâ”€ â†’ Inventory Service (slow?)
â”œâ”€ â†’ Notification Service (slow?)
â””â”€ Response returned

Which one is slow?
â”œâ”€ Check logs from Auth? Unclear
â”œâ”€ Check logs from Order? Unclear
â”œâ”€ Check logs from Payment? Unclear
â”œâ”€ Check metrics? All look OK
â””â”€ No idea which is bottleneck!

SOLUTION: Distributed Trace

One trace request_id=abc123:
â”œâ”€ Gateway: 10ms
â”œâ”€ Auth: 5ms
â”œâ”€ Order: 50ms â† BOTTLENECK!
â”œâ”€ Payment: 10ms
â”œâ”€ Inventory: 5ms
â”œâ”€ Notification: 10ms
â””â”€ Total: 90ms

Immediately see: Order service is slow!
```

### Trace Structure

```
TRACE (Request_ID = abc123):
â””â”€ Root Span: GET /checkout (0-100ms)
   â”œâ”€ Child: Auth Service (5-10ms)
   â”‚  â”œâ”€ Check token: 2ms
   â”‚  â”œâ”€ Query cache: 2ms
   â”‚  â””â”€ Return result: 1ms
   â”‚
   â”œâ”€ Child: Order Service (40-60ms) â† SLOW
   â”‚  â”œâ”€ Create order: 20ms
   â”‚  â”œâ”€ Query database: 35ms â† SLOWEST
   â”‚  â””â”€ Validate: 5ms
   â”‚
   â”œâ”€ Child: Payment Service (10-25ms)
   â”‚  â”œâ”€ Process payment: 15ms
   â”‚  â””â”€ Confirm: 5ms
   â”‚
   â””â”€ Child: Notification (5-15ms)
      â””â”€ Send email: 10ms

SPAN (Individual operation):
â”œâ”€ Trace_ID: abc123 (parent request)
â”œâ”€ Span_ID: auth_001 (unique to this call)
â”œâ”€ Parent_Span_ID: root (who called this)
â”œâ”€ Service: auth-service
â”œâ”€ Operation: check_token
â”œâ”€ Start: 5ms
â”œâ”€ Duration: 10ms
â”œâ”€ Tags: token_valid=true, cache_hit=true
â”œâ”€ Logs: ["Started", "Cache hit", "Returned"]
â””â”€ Status: OK

TAGS vs LOGS:

Tags (Structured):
â”œâ”€ Key-value pairs
â”œâ”€ user_id = 123
â”œâ”€ cache_hit = true
â”œâ”€ error_code = null
â””â”€ Queryable

Logs (Unstructured):
â”œâ”€ Text messages
â”œâ”€ "Starting operation"
â”œâ”€ "Querying database"
â”œâ”€ "Operation complete"
â””â”€ Not queryable
```

### Trace Propagation

```
REQUEST WITH TRACE CONTEXT:

Client â†’ API Gateway:
â”œâ”€ Header: X-Trace-ID: abc123
â”œâ”€ Header: X-Span-ID: gateway_001
â””â”€ Header: X-Parent-Span-ID: null

API Gateway â†’ Auth Service:
â”œâ”€ Header: X-Trace-ID: abc123 (same!)
â”œâ”€ Header: X-Span-ID: auth_001 (new!)
â”œâ”€ Header: X-Parent-Span-ID: gateway_001 (who called)
â””â”€ Headers passed through!

Auth Service â†’ Cache:
â”œâ”€ Header: X-Trace-ID: abc123 (same!)
â”œâ”€ Header: X-Span-ID: cache_001
â”œâ”€ Header: X-Parent-Span-ID: auth_001
â””â”€ Same trace through whole request!

Result: One trace_id follows entire request!
```

### Sampling

```
PROBLEM: Too many traces

Your system:
â”œâ”€ 1 million requests/second
â”œâ”€ Each generates 1 trace
â”œâ”€ 86.4 billion traces/day

Storage:
â”œâ”€ Each trace: ~10KB
â”œâ”€ 86.4B Ã— 10KB = 864 TB/day!
â”œâ”€ Storage: Impossible
â””â”€ Costs: Astronomical

SOLUTION: Sampling

Sample 1 in 100:
â”œâ”€ Trace 1% of requests
â”œâ”€ Store: 864GB/day (vs 864TB)
â”œâ”€ Cost: 100x reduction!

Sampling strategies:

Uniform sampling:
â”œâ”€ Trace 1% of all requests
â”œâ”€ Fast requests: Maybe traced
â”œâ”€ Slow requests: Maybe not traced
â””â”€ Issue: Miss rare slow requests!

Adaptive sampling:
â”œâ”€ Slow requests (> 100ms): Always trace
â”œâ”€ Normal requests (< 100ms): Trace 10%
â”œâ”€ Ensures: Slow requests captured
â””â”€ Better!

Error sampling:
â”œâ”€ Errors: Always trace
â”œâ”€ Success: Trace 1%
â””â”€ Debug errors, understand normal flow
```

---

## ğŸ”¬ Advanced Explanation

### OpenTelemetry Standard

```
PROBLEM: Too many tracing standards

Before OpenTelemetry:
â”œâ”€ Jaeger (Uber)
â”œâ”€ Zipkin (Twitter)
â”œâ”€ DataDog (proprietary)
â”œâ”€ New Relic (proprietary)
â”œâ”€ AWS X-Ray (AWS)
â””â”€ Different formats, hard to switch!

SOLUTION: OpenTelemetry

Unified standard:
â”œâ”€ One instrumentation
â”œâ”€ Multiple backends
â”œâ”€ Switch without code change!

Architecture:

Application
â”œâ”€ Uses OpenTelemetry SDK
â”œâ”€ Generates traces/metrics/logs
â””â”€ Sends to Collector

Collector (OTel Collector)
â”œâ”€ Receives data
â”œâ”€ Processes/filters/samples
â”œâ”€ Sends to backend

Backend
â”œâ”€ Jaeger
â”œâ”€ Datadog
â”œâ”€ Honeycomb
â”œâ”€ GCP Cloud Trace
â””â”€ Any OTel-compatible backend

Benefit:
âœ… No vendor lock-in
âœ… Switch backends easily
âœ… Standard format
âœ… Industry standard
```

### Trace Analysis

```
LATENCY BREAKDOWN:

Request timeline:
â”‚
â”œâ”€ Gateway: 5ms (overhead)
â”œâ”€ Auth: 10ms (token check)
â”œâ”€ Order: 50ms (main work)
â”‚  â”œâ”€ Validate: 5ms
â”‚  â”œâ”€ DB Query: 40ms â† SLOWEST!
â”‚  â””â”€ Save: 5ms
â”œâ”€ Payment: 15ms
â””â”€ Notify: 5ms

Total: 85ms

Analysis:
â”œâ”€ Where is latency? DB Query (40ms = 47%)
â”œâ”€ Can we optimize? Cache results?
â”œâ”€ Can we parallelize? Payment + Notify?
â””â”€ Expected: 80ms â†’ Actual: 85ms â†’ Close!

ERROR TRACING:

Request:
â”œâ”€ Gateway: OK
â”œâ”€ Auth: OK
â”œâ”€ Order: OK
â”œâ”€ Payment: ERROR âŒ
â”‚  â””â”€ Exception: "Card declined"
â”œâ”€ Notify: SKIPPED (due to error)
â””â”€ Response: 402 (Payment required)

Trace shows:
â”œâ”€ Where error occurred: Payment service
â”œâ”€ What error: Card declined
â”œâ”€ When: 50ms into request
â”œâ”€ Which service to debug: Payment
â””â”€ Fast root cause analysis!
```

---

## ğŸ Python Code Example

### âŒ Without Tracing (No Visibility)

```python
# ===== NO DISTRIBUTED TRACING =====

from flask import Flask
import requests

app = Flask(__name__)

@app.route('/checkout', methods=['POST'])
def checkout():
    """Checkout - no tracing"""
    
    # Call auth service
    auth_resp = requests.get('http://auth-service/verify')
    
    # Call order service
    order_resp = requests.post('http://order-service/create', json=request.json)
    
    # Call payment service
    payment_resp = requests.post('http://payment-service/process')
    
    # Combine responses
    return {'status': 'success'}

# Problems:
# âŒ No trace following request
# âŒ No visibility into service calls
# âŒ Can't see which service is slow
# âŒ Logs from each service not correlated
```

### âœ… With Basic Tracing (Manual)

```python
# ===== BASIC TRACING =====

from flask import Flask, request
import requests
import uuid

app = Flask(__name__)

@app.route('/checkout', methods=['POST'])
def checkout():
    """Checkout with basic tracing"""
    
    # Create trace ID
    trace_id = str(uuid.uuid4())
    span_id = 'checkout_001'
    
    print(f"[{trace_id}] Starting checkout")
    
    # Call auth service (pass trace context)
    headers = {
        'X-Trace-ID': trace_id,
        'X-Span-ID': 'auth_001',
        'X-Parent-Span-ID': span_id
    }
    auth_resp = requests.get('http://auth-service/verify', headers=headers)
    print(f"[{trace_id}] Auth complete")
    
    # Call order service
    headers['X-Span-ID'] = 'order_001'
    order_resp = requests.post('http://order-service/create', json=request.json, headers=headers)
    print(f"[{trace_id}] Order complete")
    
    # Call payment service
    headers['X-Span-ID'] = 'payment_001'
    payment_resp = requests.post('http://payment-service/process', headers=headers)
    print(f"[{trace_id}] Payment complete")
    
    print(f"[{trace_id}] Checkout finished")
    
    return {'status': 'success', 'trace_id': trace_id}

# Benefits:
# âœ… Can correlate logs via trace_id
# âœ“ Not ideal (manual, error-prone)
```

### âœ… With OpenTelemetry (Production)

```python
# ===== OPENTELEMETRY TRACING =====

from flask import Flask, request
import requests
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

app = Flask(__name__)

# Configure Jaeger exporter
jaeger_exporter = JaegerExporter(
    agent_host_name='localhost',
    agent_port=6831,
)

# Set up tracing
trace.set_tracer_provider(TracerProvider())
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# Auto-instrument Flask and requests
FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()

tracer = trace.get_tracer(__name__)

@app.route('/checkout', methods=['POST'])
def checkout():
    """Checkout with automatic tracing"""
    
    with tracer.start_as_current_span("checkout") as span:
        # Set tags
        span.set_attribute("user_id", request.json.get('user_id'))
        span.set_attribute("amount", request.json.get('amount'))
        
        # These calls are automatically traced!
        # OpenTelemetry intercepts requests
        
        auth_resp = requests.get('http://auth-service/verify')
        order_resp = requests.post('http://order-service/create', json=request.json)
        payment_resp = requests.post('http://payment-service/process')
        
        span.set_attribute("status", "success")
        
        return {'status': 'success'}

# Benefits:
# âœ… Automatic tracing of requests
# âœ… Automatic span creation
# âœ… Zero overhead (sampling)
# âœ… Easy context propagation
# âœ… Production-ready
```

### âœ… Custom Spans (Advanced)

```python
# ===== CUSTOM SPANS =====

from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode
import time

tracer = trace.get_tracer(__name__)

def slow_database_query(query):
    """Database query with tracing"""
    
    with tracer.start_as_current_span("db_query") as span:
        # Record parameters
        span.set_attribute("query", query)
        span.set_attribute("db", "postgres")
        
        try:
            start = time.time()
            
            # Execute query
            result = execute_query(query)
            
            duration = time.time() - start
            span.set_attribute("duration_ms", duration * 1000)
            span.set_attribute("rows", len(result))
            
            # Add event log
            span.add_event("query_complete")
            
            return result
        
        except Exception as e:
            # Record error
            span.set_attribute("error", True)
            span.set_attribute("error.type", type(e).__name__)
            span.set_attribute("error.message", str(e))
            span.set_status(Status(StatusCode.ERROR, str(e)))
            
            raise

def create_order(user_id, items):
    """Create order with multiple spans"""
    
    with tracer.start_as_current_span("create_order") as root_span:
        root_span.set_attribute("user_id", user_id)
        root_span.set_attribute("items_count", len(items))
        
        # Validate
        with tracer.start_as_current_span("validate_items"):
            validate_items(items)
        
        # Query inventory
        with tracer.start_as_current_span("check_inventory"):
            for item in items:
                with tracer.start_as_current_span("check_item"):
                    check_stock(item)
        
        # Save to database
        with tracer.start_as_current_span("save_order"):
            order_id = slow_database_query(
                f"INSERT INTO orders (user_id, items) VALUES ({user_id}, '{items}')"
            )
        
        root_span.set_attribute("order_id", order_id)
        
        return order_id

# Trace visualization in Jaeger:
#
# create_order (root)
# â”œâ”€ validate_items (5ms)
# â”œâ”€ check_inventory (30ms)
# â”‚  â”œâ”€ check_item (item1) (10ms)
# â”‚  â”œâ”€ check_item (item2) (10ms)
# â”‚  â””â”€ check_item (item3) (10ms)
# â””â”€ save_order (45ms)
#    â””â”€ db_query (40ms) â† slowest!
#
# Total: 80ms
```

---

## ğŸ’¡ Mini Project: "Build Tracing System"

### Phase 1: Basic Tracing â­

**Requirements:**
- Manual trace context propagation
- Jaeger setup
- View traces in Jaeger UI
- Multiple spans per request

---

### Phase 2: OpenTelemetry â­â­

**Requirements:**
- Auto-instrumentation
- Custom spans
- Error tracking
- Performance metrics

---

### Phase 3: Production Ready â­â­â­

**Requirements:**
- Sampling strategies
- Multiple backends
- Trace correlation with logs
- Alert on slow traces

---

## âš–ï¸ Observability Pillars Complete

| Pillar | Tool | Use Case |
|--------|------|----------|
| **Metrics** | Prometheus | Trending, alerting |
| **Logs** | ELK, Loki | Debugging, events |
| **Traces** | Jaeger | Performance, dependencies |

---

## âŒ Common Mistakes

### Mistake 1: No Sampling

```python
# âŒ Trace 100% of requests
# Storage explodes, costs astronomical

# âœ… Intelligent sampling
# Sample slow requests: 100%
# Sample normal: 10%
# Sample errors: 100%
```

### Mistake 2: Not Propagating Context

```python
# âŒ Trace context lost at service boundary
request_to_other_service()  # No trace context!

# âœ… Propagate headers
headers = {
    'X-Trace-ID': trace_id,
    'X-Span-ID': span_id,
    'X-Parent-Span-ID': parent_span_id
}
request_to_other_service(headers=headers)
```

### Mistake 3: Too Many Spans

```python
# âŒ Create span for every operation
with tracer.start_span("increment_i"): i += 1
with tracer.start_span("increment_j"): j += 1
# Creates thousands of spans!

# âœ… Only span important operations
with tracer.start_span("main_loop"):
    for i in range(1000):
        i += 1  # Don't span this
```

---

## ğŸ“š Additional Resources

**Jaeger:**
- [Jaeger](https://www.jaegertracing.io/)
- [Jaeger Tutorial](https://medium.com/jaegertracing/jaeger-tracing-tutorial-fbb1e3fc5faf)

**OpenTelemetry:**
- [OpenTelemetry](https://opentelemetry.io/)
- [OTel Python SDK](https://opentelemetry.io/docs/instrumentation/python/)

**Tracing:**
- [Distributed Tracing](https://opentracing.io/)
- [Trace Context](https://www.w3.org/TR/trace-context/)

---

## ğŸ¯ Before You Leave

**Can you answer these?**

1. **Trace vs span?**
   - Answer: Trace is request; span is service call

2. **Why trace context propagation?**
   - Answer: Correlate across service boundaries

3. **Sampling strategies?**
   - Answer: Uniform, adaptive, error-based

4. **OpenTelemetry benefit?**
   - Answer: Vendor-neutral standard, switch backends

5. **When to use tracing vs logging?**
   - Answer: Tracing for path; logging for details

**If you got these right, you're ready for real systems!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **Microservices:** "I call 50 other services!"
>
> **Request:** "Where am I?"
>
> **Microservices:** "No idea, could be anywhere"
>
> **Distributed Trace:** "Found you!"
>
> **Request:** "Finally! Who was slow?"
>
> **Distributed Trace:** "Database, but we already knew that"
>
> **Everyone:** "At least now we can prove it!" ğŸ“Š

---

[â† Back to Main](../README.md) | [Previous: Metrics](45-metrics-prometheus.md) | [Next: URL Shortener](../case-studies/47-url-shortener.md)

---

**Last Updated:** November 11, 2025  
**Difficulty:** â­â­â­â­ Advanced (observability/microservices)  
**Time to Read:** 26 minutes  
**Time to Implement:** 5-8 hours per phase  

---

*Distributed Tracing: Finally understanding what your microservices are doing.* ğŸš€