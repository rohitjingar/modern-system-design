# 44. Logging, Monitoring, and Alerting

You deploy to production. Everything works perfectly. Then you get paged at 3 AM. "The system is down!" You check logs. Nothing. You check metrics. All look fine. You check alerts. Silent. Turns out: logging broke, metrics weren't being collected, and alerts were misconfigured. The only working monitoring was angry customers calling support. Welcome to observability! ğŸ”¥ğŸ“Š

[â† Back to Main](../README.md) | [Previous: SSL/TLS & HTTPS](43-ssl-tls-https.md) | [Next: Metrics (Prometheus, Grafana) â†’](45-metrics-prometheus.md)

---

## ğŸ¯ Quick Summary

**Logging** records events (errors, requests, debugging info). **Monitoring** tracks system health (CPU, memory, latency). **Alerting** notifies when problems occur (page engineers). Together = Observability. **ELK Stack** (Elasticsearch, Logstash, Kibana) for logs. **Prometheus + Grafana** for metrics. **PagerDuty** for alerts. Netflix generates petabytes of logs daily. Google uses similar systems. Trade-off: storage (massive), alert fatigue (too many alerts), cost (infrastructure).

Think of it as: **Logging = History, Monitoring = Now, Alerting = Action**

---

## ğŸŒŸ Beginner Explanation

### Three Pillars of Observability

```
LOGS (What happened?):

Application logs:
â”œâ”€ 2025-11-30 10:00:01 INFO: User 123 logged in
â”œâ”€ 2025-11-30 10:00:05 DEBUG: Query took 45ms
â”œâ”€ 2025-11-30 10:00:10 ERROR: Payment failed - card declined
â”œâ”€ 2025-11-30 10:00:15 WARN: Rate limit approaching
â””â”€ Text-based, timestamped events

Use cases:
â”œâ”€ Debugging: "Why did this user's order fail?"
â”œâ”€ Audit: "Who deleted this record?"
â”œâ”€ Investigation: "What happened before the crash?"
â””â”€ Root cause analysis

Levels:
â”œâ”€ DEBUG: Detailed info for developers
â”œâ”€ INFO: General informational messages
â”œâ”€ WARN: Warning, something might be wrong
â”œâ”€ ERROR: Error occurred, but app continues
â””â”€ FATAL: Critical error, app crashed


METRICS (How is it performing?):

System metrics:
â”œâ”€ CPU usage: 45%
â”œâ”€ Memory usage: 2.1 GB / 8 GB
â”œâ”€ Disk I/O: 150 MB/s
â”œâ”€ Network: 50 Mbps
â””â”€ Quantitative measurements

Application metrics:
â”œâ”€ Requests per second: 3,500
â”œâ”€ Error rate: 0.5%
â”œâ”€ P99 latency: 120ms
â”œâ”€ Active users: 10,000
â””â”€ Business metrics

Use cases:
â”œâ”€ Performance: "Is the system slow?"
â”œâ”€ Capacity: "Do we need more servers?"
â”œâ”€ Trending: "Is traffic growing?"
â””â”€ SLA monitoring: "Are we meeting our target?"


ALERTS (What needs attention?):

Alert conditions:
â”œâ”€ CPU > 80% for 5 minutes â†’ Alert
â”œâ”€ Error rate > 1% â†’ Alert
â”œâ”€ Latency > 500ms â†’ Alert
â”œâ”€ Disk space < 10% â†’ Alert
â””â”€ Service down â†’ Alert

Alert channels:
â”œâ”€ PagerDuty: Wake up on-call engineer
â”œâ”€ Slack: Team notification
â”œâ”€ Email: Non-urgent issues
â”œâ”€ SMS: Critical only
â””â”€ Phone call: Emergency

Use cases:
â”œâ”€ Incident response: "System is down, fix it!"
â”œâ”€ Proactive: "Disk filling up, add space"
â”œâ”€ SLA breach: "Latency too high, investigate"
â””â”€ Security: "Unusual activity detected"
```

### Log Aggregation

```
PROBLEM: Logs scattered across servers

Without aggregation:
â”œâ”€ Server 1: /var/log/app.log
â”œâ”€ Server 2: /var/log/app.log
â”œâ”€ Server 3: /var/log/app.log
â”œâ”€ ... (100 servers)
â””â”€ Must SSH to each server to read logs!

Debugging:
â”œâ”€ User reports error at 10:05 AM
â”œâ”€ Which server handled it? Unknown
â”œâ”€ SSH to server 1: grep "error" app.log
â”œâ”€ SSH to server 2: grep "error" app.log
â”œâ”€ ... repeat 100 times
â””â”€ Takes hours!


SOLUTION: Centralized logging (ELK Stack)

Architecture:
â”œâ”€ Server 1 â†’ Logs â†’ Filebeat â†’ Logstash
â”œâ”€ Server 2 â†’ Logs â†’ Filebeat â†’ Logstash
â”œâ”€ Server 3 â†’ Logs â†’ Filebeat â†’ Logstash
â”œâ”€ Logstash: Parse, filter, enrich
â”œâ”€ Elasticsearch: Store, index logs
â””â”€ Kibana: Search, visualize logs

Query:
â”œâ”€ Search: "ERROR" AND timestamp:[10:04 TO 10:06]
â”œâ”€ Results: All errors across all servers
â”œâ”€ Found: Server 47 had the error
â”œâ”€ Time: < 1 second!
â””â”€ Fast debugging!


ELK STACK:

Elasticsearch:
â”œâ”€ Stores logs as documents
â”œâ”€ Full-text search (fast!)
â”œâ”€ Scales horizontally
â””â”€ Think: Google for logs

Logstash:
â”œâ”€ Collects logs from servers
â”œâ”€ Parses: Extract fields from text
â”œâ”€ Filters: Enrich with metadata
â”œâ”€ Sends to Elasticsearch
â””â”€ Think: Log processing pipeline

Kibana:
â”œâ”€ Web UI for searching logs
â”œâ”€ Create dashboards
â”œâ”€ Visualize trends
â”œâ”€ Set up alerts
â””â”€ Think: UI for Elasticsearch
```

### Monitoring & Metrics

```
PROMETHEUS WORKFLOW:

1. Application exposes metrics:
   GET /metrics
   Returns:
   â”œâ”€ requests_total{path="/api"} 1000
   â”œâ”€ request_duration_seconds 0.045
   â”œâ”€ memory_bytes 1000000
   â””â”€ errors_total{code="500"} 5

2. Prometheus scrapes metrics:
   â”œâ”€ Every 15 seconds (configurable)
   â”œâ”€ From all targets
   â”œâ”€ Stores in time-series database
   â””â”€ Can query historical data

3. Grafana visualizes:
   â”œâ”€ Dashboard: CPU, memory, requests
   â”œâ”€ Graphs: Time-series charts
   â”œâ”€ Real-time updates
   â””â”€ Team views health at a glance

4. Alertmanager alerts:
   â”œâ”€ Rule: cpu_usage > 80 for 5m
   â”œâ”€ Fires: When condition true
   â”œâ”€ Notifies: PagerDuty, Slack, email
   â””â”€ Engineers respond


GRAFANA DASHBOARD:

Panel 1: Request Rate
â”œâ”€ Query: rate(requests_total[5m])
â”œâ”€ Shows: Requests per second
â”œâ”€ Graph type: Line chart
â””â”€ Updates: Every 5 seconds

Panel 2: Error Rate
â”œâ”€ Query: rate(errors_total[5m]) / rate(requests_total[5m])
â”œâ”€ Shows: Error percentage
â”œâ”€ Graph type: Line chart
â”œâ”€ Alert: If > 1%

Panel 3: Latency (P99)
â”œâ”€ Query: histogram_quantile(0.99, request_duration_seconds_bucket)
â”œâ”€ Shows: 99th percentile latency
â”œâ”€ Graph type: Line chart
â””â”€ Alert: If > 500ms

Panel 4: Active Users
â”œâ”€ Query: active_users
â”œâ”€ Shows: Current logged-in users
â”œâ”€ Graph type: Gauge
â””â”€ Business metric
```

### Alert Best Practices

```
GOOD ALERT:

Alert: High Error Rate
â”œâ”€ Condition: error_rate > 1% for 5 minutes
â”œâ”€ Severity: Critical
â”œâ”€ Notification: PagerDuty (page on-call)
â”œâ”€ Runbook: Link to debugging steps
â”œâ”€ Context: Which service, what time
â””â”€ Actionable: Clear what to fix

Result:
âœ… Engineer knows what's wrong
âœ… Has steps to fix it
âœ… Can resolve quickly


BAD ALERT:

Alert: Something is wrong
â”œâ”€ Condition: Unknown
â”œâ”€ Severity: Unknown
â”œâ”€ Notification: Email to everyone
â”œâ”€ Runbook: None
â”œâ”€ Context: None
â””â”€ Not actionable

Result:
âŒ Engineer confused
âŒ No idea what to fix
âŒ Wastes time investigating
âŒ Alert fatigue (ignore future alerts)


ALERT FATIGUE:

Problem:
â”œâ”€ Too many alerts (50/day)
â”œâ”€ Most are false positives
â”œâ”€ Engineers ignore alerts
â”œâ”€ Real issue: Missed!
â””â”€ System down for hours

Solution:
â”œâ”€ Fewer, better alerts
â”œâ”€ Only alert on actionable issues
â”œâ”€ Group similar alerts
â”œâ”€ Escalation policies
â””â”€ Regular review and tuning
```

---

## ğŸ”¬ Advanced Explanation

### Structured Logging

```
UNSTRUCTURED LOGGING (Bad):

Log entry:
"User 123 logged in from IP 192.168.1.1 at 10:00:01"

Problems:
â”œâ”€ Text parsing required
â”œâ”€ Hard to query (regex)
â”œâ”€ Inconsistent format
â”œâ”€ No metadata
â””â”€ Slow searches


STRUCTURED LOGGING (Good):

Log entry (JSON):
{
  "timestamp": "2025-11-30T10:00:01Z",
  "level": "INFO",
  "message": "User logged in",
  "user_id": 123,
  "ip_address": "192.168.1.1",
  "session_id": "abc123",
  "trace_id": "xyz789"
}

Benefits:
âœ… Easy to parse
âœ… Fast queries (indexed fields)
âœ… Consistent format
âœ… Rich metadata
âœ… Correlation (trace_id)

Query examples:
â”œâ”€ Find all logs for user_id=123
â”œâ”€ Find all logs from IP=192.168.1.1
â”œâ”€ Find all logs with trace_id=xyz789
â””â”€ Fast and precise!
```

### Log Sampling

```
PROBLEM: Too many logs

High-traffic service:
â”œâ”€ 10,000 requests/second
â”œâ”€ Each request: 5 log lines
â”œâ”€ Total: 50,000 logs/second
â”œâ”€ Per day: 4.3 billion logs
â”œâ”€ Storage: 4.3 TB/day (uncompressed)
â””â”€ Cost: Astronomical!


SOLUTION: Intelligent sampling

Strategy 1: Sample by percentage
â”œâ”€ Log 10% of requests
â”œâ”€ Storage: 430 GB/day
â”œâ”€ Cost: 90% reduction
â””â”€ Issue: Miss rare errors

Strategy 2: Sample by error
â”œâ”€ Log all errors (100%)
â”œâ”€ Log 10% of success (sample)
â”œâ”€ Storage: Low
â”œâ”€ Benefit: Catch all problems!
â””â”€ Recommended

Strategy 3: Adaptive sampling
â”œâ”€ Slow requests (>100ms): Log 100%
â”œâ”€ Normal requests: Log 10%
â”œâ”€ Fast requests (<10ms): Log 1%
â””â”€ Optimal balance
```

### Distributed Tracing Integration

```
CORRELATION: Logs + Traces

Request flow:
â”œâ”€ API Gateway (trace_id=abc123)
â”‚  â””â”€ Log: "Request received" (trace_id=abc123)
â”œâ”€ Auth Service (trace_id=abc123, span_id=auth_001)
â”‚  â””â”€ Log: "Token validated" (trace_id=abc123)
â”œâ”€ Order Service (trace_id=abc123, span_id=order_001)
â”‚  â””â”€ Log: "Order created" (trace_id=abc123)
â””â”€ Payment Service (trace_id=abc123, span_id=pay_001)
   â””â”€ Log: "Payment processed" (trace_id=abc123)

Benefit:
â”œâ”€ Trace shows: Which services involved
â”œâ”€ Logs show: What happened in each
â”œâ”€ Combined: Complete picture
â””â”€ Fast debugging!

Search:
â”œâ”€ Query logs: trace_id=abc123
â”œâ”€ Returns: All logs for this request
â”œâ”€ Across all services
â””â”€ Full story!
```

---

## ğŸ Python Code Example

### âŒ Without Proper Logging

```python
# ===== NO LOGGING =====

from flask import Flask

app = Flask(__name__)

@app.route('/api/orders', methods=['POST'])
def create_order():
    """Create order - no logging"""
    
    try:
        order = process_order(request.json)
        return {'order_id': order.id}
    except Exception as e:
        # Silent failure!
        return {'error': 'Failed'}, 500

# Problems:
# âŒ No logs
# âŒ Can't debug issues
# âŒ No visibility
# âŒ No metrics
```

### âœ… With Structured Logging

```python
# ===== WITH STRUCTURED LOGGING =====

from flask import Flask, request
import logging
import json
from datetime import datetime

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s'
)
logger = logging.getLogger(__name__)

class StructuredLogger:
    """Structured JSON logger"""
    
    def __init__(self, logger):
        self.logger = logger
    
    def log(self, level, message, **kwargs):
        """Log structured JSON"""
        
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level,
            'message': message,
            **kwargs
        }
        
        self.logger.log(
            getattr(logging, level),
            json.dumps(log_entry)
        )

structured_logger = StructuredLogger(logger)

@app.route('/api/orders', methods=['POST'])
def create_order():
    """Create order with logging"""
    
    user_id = request.json.get('user_id')
    
    structured_logger.log(
        'INFO',
        'Order creation started',
        user_id=user_id,
        endpoint='/api/orders',
        method='POST'
    )
    
    try:
        start_time = time.time()
        
        order = process_order(request.json)
        
        duration = time.time() - start_time
        
        structured_logger.log(
            'INFO',
            'Order created successfully',
            user_id=user_id,
            order_id=order.id,
            duration_ms=duration * 1000
        )
        
        return {'order_id': order.id}
    
    except Exception as e:
        structured_logger.log(
            'ERROR',
            'Order creation failed',
            user_id=user_id,
            error=str(e),
            error_type=type(e).__name__
        )
        
        return {'error': 'Failed'}, 500

# Log output (JSON):
# {"timestamp": "2025-11-30T10:00:01Z", "level": "INFO", 
#  "message": "Order created successfully", "user_id": 123, 
#  "order_id": 456, "duration_ms": 45.2}

# Benefits:
# âœ… Structured JSON logs
# âœ… Easy to parse and query
# âœ… Rich metadata
# âœ… Production-ready
```

### âœ… Production Observability Stack

```python
# ===== PRODUCTION OBSERVABILITY =====

from flask import Flask, request
import logging
from prometheus_client import Counter, Histogram, generate_latest
from opentelemetry import trace
import json

app = Flask(__name__)

# Structured logging
structured_logger = StructuredLogger(logging.getLogger(__name__))

# Prometheus metrics
requests_total = Counter(
    'requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'request_duration_seconds',
    'Request duration',
    ['endpoint']
)

# OpenTelemetry tracing
tracer = trace.get_tracer(__name__)

class ObservabilityMiddleware:
    """Complete observability: logs + metrics + traces"""
    
    def __init__(self, app):
        self.app = app
    
    def __call__(self, environ, start_response):
        """Wrap request with observability"""
        
        # Start trace
        with tracer.start_as_current_span("http_request") as span:
            trace_id = span.get_span_context().trace_id
            
            # Log request start
            structured_logger.log(
                'INFO',
                'Request received',
                method=environ['REQUEST_METHOD'],
                path=environ['PATH_INFO'],
                trace_id=format(trace_id, '032x')
            )
            
            # Track metrics
            start_time = time.time()
            
            def custom_start_response(status, headers):
                # Extract status code
                status_code = int(status.split()[0])
                
                # Record metrics
                duration = time.time() - start_time
                request_duration.labels(
                    endpoint=environ['PATH_INFO']
                ).observe(duration)
                
                requests_total.labels(
                    method=environ['REQUEST_METHOD'],
                    endpoint=environ['PATH_INFO'],
                    status=status_code
                ).inc()
                
                # Log completion
                structured_logger.log(
                    'INFO',
                    'Request completed',
                    method=environ['REQUEST_METHOD'],
                    path=environ['PATH_INFO'],
                    status=status_code,
                    duration_ms=duration * 1000,
                    trace_id=format(trace_id, '032x')
                )
                
                return start_response(status, headers)
            
            return self.app(environ, custom_start_response)

# Apply middleware
app.wsgi_app = ObservabilityMiddleware(app.wsgi_app)

@app.route('/metrics')
def metrics():
    """Prometheus metrics endpoint"""
    return generate_latest()

# Benefits:
# âœ… Logs: Structured JSON with trace_id
# âœ… Metrics: Prometheus-compatible
# âœ… Traces: Distributed tracing
# âœ… Complete observability
# âœ… Production-ready
```

---

## ğŸ’¡ Mini Project: "Build Observability"

### Phase 1: Logging â­

**Requirements:**
- Structured logging (JSON)
- Log levels (DEBUG, INFO, ERROR)
- Log aggregation (ELK)
- Search and filter

---

### Phase 2: Monitoring â­â­

**Requirements:**
- Prometheus metrics
- Grafana dashboards
- Real-time visualization
- Historical trending

---

### Phase 3: Complete Stack â­â­â­

**Requirements:**
- Logs + Metrics + Traces
- Alert rules (Alertmanager)
- PagerDuty integration
- Runbooks and documentation

---

## âš–ï¸ Observability Tools

| Tool | Purpose | Pros | Cons |
|------|---------|------|------|
| **ELK Stack** | Logging | Powerful search | Complex setup |
| **Prometheus** | Metrics | Time-series | Storage limits |
| **Jaeger** | Tracing | Distributed | Overhead |
| **Datadog** | All-in-one | Easy | Expensive |

---

## âŒ Common Mistakes

### Mistake 1: Logging Everything

```python
# âŒ Log every operation
for i in range(1000):
    logger.info(f"Processing item {i}")
# Generates 1000 logs (spam!)

# âœ… Log milestones
logger.info(f"Processing {len(items)} items")
# Process items...
logger.info(f"Processing complete")
```

### Mistake 2: No Alert Runbooks

```python
# âŒ Alert without context
Alert: "CPU high"
# What do I do?

# âœ… Alert with runbook
Alert: "CPU high - Check process consuming CPU, 
        restart if necessary. Runbook: wiki.com/cpu-runbook"
```

### Mistake 3: Ignoring Log Storage Costs

```python
# âŒ Keep all logs forever
# Storage grows infinitely
# Costs explode

# âœ… Retention policy
# Keep 30 days in hot storage
# Archive 90 days in cold storage
# Delete after 1 year
```

---

## ğŸ“š Additional Resources

**Logging:**
- [ELK Stack](https://www.elastic.co/elk-stack)
- [Structured Logging](https://www.structuredlogging.com/)

**Monitoring:**
- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)

**Alerting:**
- [PagerDuty](https://www.pagerduty.com/)
- [Alert Best Practices](https://docs.pagerduty.com/docs/best-practices)


---

## ğŸ¯ Before You Leave

**Can you answer these?**

1. **Logs vs metrics vs traces?**
   - Answer: Logs = what happened; Metrics = how performing; Traces = request path

2. **Why structured logging?**
   - Answer: Easy to parse, query, and analyze

3. **Alert fatigue?**
   - Answer: Too many alerts, engineers ignore them

4. **ELK Stack components?**
   - Answer: Elasticsearch, Logstash, Kibana

5. **Good alert characteristics?**
   - Answer: Actionable, has runbook, provides context

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **3 AM Page:** "System is down!"
>
> **Engineer:** Checks logs â†’ Empty
>
> **Engineer:** Checks metrics â†’ All good
>
> **Engineer:** Checks alerts â†’ Silent
>
> **Engineer:** "How do I even debug this?"
>
> **Boss:** "That's why we pay you the big bucks!"
>
> **Engineer:** "I need better observability..." ğŸ˜¤

---

[â† Back to Main](../README.md) | [Previous: SSL/TLS & HTTPS](43-ssl-tls-https.md) | [Next: Metrics (Prometheus, Grafana) â†’](45-metrics-prometheus.md)

---

**Last Updated:** November 30, 2025  
**Difficulty:** â­â­â­ Intermediate-Advanced (operations)  
**Time to Read:** 25 minutes  
**Time to Implement:** 6-10 hours per phase  

---

*Logging, Monitoring, and Alerting: Know what's happening before your users tell you.* ğŸš€