# 31. Data Pipelines & Stream Processing

Batch processing is like doing laundry once a week: you pile everything up and process it all at once. Stream processing is like washing each sock individually as soon as it gets dirty. One is efficient but slow. The other is real-time but exhausting. You'll end up doing both because your users want instant results and your servers need breaks. ğŸ§¦âš¡

[â† Back to Main](../README.md) | [Previous: CQRS](30-cqrs.md) | [Next: Edge Computing â†’](32-edge-computing.md)

---

## ğŸ¯ Quick Summary

**Data Pipelines** move data from source to destination, transforming it along the way (ETL: Extract, Transform, Load). **Stream Processing** processes data in real-time as it arrives (events, logs, metrics). Batch: processes daily/hourly (Hadoop, Spark). Stream: processes immediately (Kafka Streams, Flink, Spark Streaming). Netflix uses pipelines for analytics. Uber uses streams for real-time pricing. Trade-off: batch is simpler, stream is complex but real-time.

Think of it as: **Batch = Process Later, Stream = Process Now**

---

## ğŸŒŸ Beginner Explanation

### Batch vs Stream Processing

**BATCH PROCESSING (Process in Bulk):**

```
Scenario: Daily analytics report

00:00 (midnight): Collect all day's data
â”œâ”€ 1 million user events
â”œâ”€ 500k transactions
â”œâ”€ 2 million page views
â””â”€ Store in data lake

01:00: Start batch job
â”œâ”€ Read all data
â”œâ”€ Aggregate by user
â”œâ”€ Calculate metrics
â”œâ”€ Write to analytics DB
â””â”€ Takes 2 hours

03:00: Report ready
â”œâ”€ Users see yesterday's data
â”œâ”€ 3-hour delay acceptable
â””â”€ Efficient processing

Pros:
âœ… Simple to build
âœ… Efficient (process once)
âœ… Easy to debug
âœ… Cheap (off-peak hours)

Cons:
âŒ Not real-time (hours delay)
âŒ All or nothing (if fails: reprocess all)
âŒ High latency (wait for batch)
```

**STREAM PROCESSING (Process Immediately):**

```
Scenario: Real-time fraud detection

User makes payment:
â”œâ”€ Event: payment.initiated
â”œâ”€ Stream processor receives immediately
â”œâ”€ Check: Is amount > $10,000?
â”œâ”€ Check: Is user in risky country?
â”œâ”€ Check: Has user paid before?
â”œâ”€ Decision: Approve or block
â””â”€ Result in < 100ms

Every event processed individually
No waiting for batch

Pros:
âœ… Real-time (< 1 second)
âœ… Immediate results
âœ… Actionable insights now

Cons:
âŒ Complex (stateful processing)
âŒ Expensive (always running)
âŒ Hard to debug (distributed)
```

### Data Pipeline Architecture

```
DATA PIPELINE (ETL):

EXTRACT (Get data):
â”œâ”€ Source 1: Application database
â”œâ”€ Source 2: User logs
â”œâ”€ Source 3: External API
â”œâ”€ Source 4: File uploads
â””â”€ Extract every hour

TRANSFORM (Clean & process):
â”œâ”€ Parse JSON
â”œâ”€ Filter invalid records
â”œâ”€ Join with user table
â”œâ”€ Aggregate metrics
â”œâ”€ Denormalize
â””â”€ Enrich with metadata

LOAD (Store):
â”œâ”€ Destination: Data warehouse (Snowflake, BigQuery)
â”œâ”€ Or: Data lake (S3)
â”œâ”€ Or: Analytics DB (ClickHouse)
â””â”€ Ready for queries

Example: E-Commerce

Extract:
â”œâ”€ Orders from PostgreSQL
â”œâ”€ Clicks from log files
â”œâ”€ User data from API

Transform:
â”œâ”€ Join orders + clicks
â”œâ”€ Calculate: conversion rate
â”œâ”€ Filter: valid orders only
â”œâ”€ Aggregate: by day/product

Load:
â”œâ”€ Write to data warehouse
â””â”€ Business analysts query
```

### Stream Processing Flow

```
STREAM PROCESSING (Real-Time):

Event Source:
â”œâ”€ User clicks (millions/sec)
â”œâ”€ Transactions (thousands/sec)
â”œâ”€ IoT sensors (billions/sec)
â””â”€ Logs (terabytes/day)

Stream Processor:
â”œâ”€ Kafka Streams
â”œâ”€ Apache Flink
â”œâ”€ Spark Streaming
â””â”€ Processes each event

Operations:
â”œâ”€ Filter (remove invalid)
â”œâ”€ Map (transform)
â”œâ”€ Aggregate (count, sum)
â”œâ”€ Join (with other streams)
â”œâ”€ Window (time-based grouping)
â””â”€ Output to sink

Sink:
â”œâ”€ Database (write results)
â”œâ”€ Dashboard (update metrics)
â”œâ”€ Alerts (trigger notifications)
â””â”€ Another stream (chaining)

Example: Twitter Trending Topics

Input: Tweets (stream)
Process:
â”œâ”€ Filter hashtags
â”œâ”€ Count per hashtag (5 min window)
â”œâ”€ Rank top 10
â”œâ”€ Update every 5 minutes
Output: Trending topics dashboard
```

---

## ğŸ”¬ Advanced Explanation

### Batch Processing Deep Dive

```
HADOOP MAPREDUCE (Classic Batch):

Job: Count words in 1TB of logs

Map Phase (Parallel):
â”œâ”€ Split file into 1000 chunks
â”œâ”€ Each chunk: 1GB
â”œâ”€ Map task per chunk
â”œâ”€ Output: (word, 1) pairs

Mapper 1: Chunk 1
â”œâ”€ "hello world" â†’ (hello, 1), (world, 1)

Mapper 2: Chunk 2
â”œâ”€ "hello again" â†’ (hello, 1), (again, 1)

... 1000 mappers total

Shuffle (Group by key):
â”œâ”€ All (hello, 1) pairs go to Reducer 1
â”œâ”€ All (world, 1) pairs go to Reducer 2
â””â”€ Network transfer (expensive!)

Reduce Phase (Aggregate):
â”œâ”€ Reducer 1: Sum all (hello, 1) â†’ (hello, 5000)
â”œâ”€ Reducer 2: Sum all (world, 1) â†’ (world, 3000)
â””â”€ Write to HDFS

Result: Word counts
Time: 30 minutes for 1TB

Limitations:
âŒ High latency (minutes to hours)
âŒ No incremental results
âŒ Must reprocess all data if fails
```

**SPARK (Modern Batch):**

```
Spark improves on Hadoop:

In-Memory Processing:
â”œâ”€ Cache data in RAM (not disk)
â”œâ”€ 10-100x faster than Hadoop
â””â”€ Reuse cached data

Lazy Evaluation:
â”œâ”€ Build execution plan
â”œâ”€ Optimize before running
â””â”€ Execute only when needed

DAG (Directed Acyclic Graph):
â”œâ”€ Multiple stages
â”œâ”€ Pipeline transformations
â”œâ”€ Efficient execution plan

Example: Word count in Spark
data = spark.read.text("hdfs://logs")
words = data.flatMap(lambda line: line.split(" "))
counts = words.groupBy("word").count()
counts.write.parquet("hdfs://output")

Result: 10x faster than Hadoop
```

### Stream Processing Deep Dive

```
KAFKA STREAMS (Real-Time):

Topology: Sequence of processing nodes

Source â†’ Processor â†’ Processor â†’ Sink
(read)   (filter)    (aggregate)  (write)

Example: Real-time analytics

Input: User clicks
â”œâ”€ Event: {user_id, page, timestamp}
â”œâ”€ 10,000 events/sec

Processor 1: Filter
â”œâ”€ Keep only product page clicks
â”œâ”€ Output: 3,000 events/sec

Processor 2: Window
â”œâ”€ Group by 5-minute window
â”œâ”€ Count clicks per product
â”œâ”€ Output: {product_id, count, window}

Processor 3: Aggregate
â”œâ”€ Maintain running totals
â”œâ”€ Update dashboard
â””â”€ Output: Real-time counts

Sink: Write to database
â”œâ”€ Update product_views table
â””â”€ Visible immediately

State Management:
â”œâ”€ Kafka Streams maintains state
â”œâ”€ Stored in RocksDB (local)
â”œâ”€ Backed up to Kafka (distributed)
â””â”€ Recovers on failure
```

**WINDOWING (Time-Based Aggregation):**

```
TUMBLING WINDOW (Fixed, Non-Overlapping):

Window size: 5 minutes
â”œâ”€ Window 1: 00:00-00:05 (closed)
â”œâ”€ Window 2: 00:05-00:10 (closed)
â”œâ”€ Window 3: 00:10-00:15 (active)
â””â”€ Each event in exactly one window

Events:
â”œâ”€ 00:01: user_id=1 â†’ Window 1
â”œâ”€ 00:04: user_id=2 â†’ Window 1
â”œâ”€ 00:06: user_id=3 â†’ Window 2
â”œâ”€ 00:09: user_id=1 â†’ Window 2
â””â”€ 00:11: user_id=2 â†’ Window 3

Aggregate:
â”œâ”€ Window 1: 2 events
â”œâ”€ Window 2: 2 events
â””â”€ Window 3: 1 event (so far)

SLIDING WINDOW (Overlapping):

Window size: 5 minutes, Slide: 1 minute
â”œâ”€ Window 1: 00:00-00:05
â”œâ”€ Window 2: 00:01-00:06 (overlaps W1)
â”œâ”€ Window 3: 00:02-00:07 (overlaps W2)
â””â”€ Event can be in multiple windows

Event at 00:03:
â”œâ”€ In Window 1 (00:00-00:05)
â”œâ”€ In Window 2 (00:01-00:06)
â”œâ”€ In Window 3 (00:02-00:07)
â””â”€ Counted 3 times!

SESSION WINDOW (Activity-Based):

Gap: 30 minutes of inactivity
â”œâ”€ User active: Extend window
â”œâ”€ User inactive 30 min: Close window
â””â”€ Dynamic window size

User 1:
â”œâ”€ 00:00: Event (start session)
â”œâ”€ 00:10: Event (extend)
â”œâ”€ 00:15: Event (extend)
â”œâ”€ 00:45: No activity (30 min gap)
â””â”€ Session closed: 00:00-00:15 (15 min)

User 2:
â”œâ”€ 00:00: Event
â”œâ”€ 01:00: Event (gap > 30 min)
â””â”€ Two sessions: 00:00, 01:00
```

### Lambda Architecture (Batch + Stream)

```
LAMBDA ARCHITECTURE (Best of Both):

Problem: Need both real-time AND accuracy

Solution: Run batch AND stream in parallel

BATCH LAYER (Accurate):
â”œâ”€ Process all historical data
â”œâ”€ Run daily (comprehensive)
â”œâ”€ Accurate but slow
â””â”€ Output: Batch view

SPEED LAYER (Real-Time):
â”œâ”€ Process recent data only
â”œâ”€ Run continuously
â”œâ”€ Fast but approximate
â””â”€ Output: Real-time view

SERVING LAYER (Merge):
â”œâ”€ Query = Batch view + Real-time view
â”œâ”€ Real-time: Last 24 hours
â”œâ”€ Batch: Everything older
â””â”€ Combined result

Example: Page view counts

Batch (accurate):
â”œâ”€ Count page views from all logs
â”œâ”€ Run once per day
â””â”€ Result: 1,000,000 views (yesterday)

Stream (real-time):
â”œâ”€ Count page views in last hour
â”œâ”€ Run continuously
â””â”€ Result: 500 views (last hour)

Query:
â”œâ”€ Total views = 1,000,000 + 500
â””â”€ Result: 1,000,500 views

Benefits:
âœ… Accuracy (batch)
âœ… Low latency (stream)
âœ… Fault tolerance (recompute from batch)
```

---

## ğŸ Python Code Example

### âŒ Without Pipeline (Manual Processing)

```python
# ===== WITHOUT PIPELINE (MANUAL) =====

import psycopg2
import json

# Manual data processing (no pipeline)
def process_orders_manually():
    """Process orders one by one (slow)"""
    
    # Connect to database
    conn = psycopg2.connect("dbname=shop")
    cursor = conn.cursor()
    
    # Get all orders
    cursor.execute("SELECT * FROM orders")
    orders = cursor.fetchall()
    
    # Process each order
    for order in orders:
        # Transform
        order_data = {
            'order_id': order[0],
            'user_id': order[1],
            'amount': order[2],
            'status': order[3]
        }
        
        # Write to analytics (manually)
        print(f"Processing order: {order_data}")
    
    print(f"Processed {len(orders)} orders")

# Problems:
# âŒ No reusability
# âŒ No parallelism
# âŒ No error handling
# âŒ Not scalable
# âŒ Manual execution
```

### âœ… Batch Processing Pipeline (Apache Spark)

```python
# ===== BATCH PROCESSING PIPELINE (SPARK) =====

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, count, avg, window

# Initialize Spark
spark = SparkSession.builder \
    .appName("OrderAnalytics") \
    .getOrCreate()

class BatchPipeline:
    """Batch processing pipeline"""
    
    def __init__(self):
        self.spark = spark
    
    def extract(self):
        """Extract: Read data from sources"""
        
        # Read from database
        orders = self.spark.read \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://localhost/shop") \
            .option("dbtable", "orders") \
            .load()
        
        # Read from log files
        logs = self.spark.read \
            .json("hdfs://logs/user_activity/*.json")
        
        return orders, logs
    
    def transform(self, orders, logs):
        """Transform: Clean and aggregate"""
        
        # Filter valid orders
        valid_orders = orders.filter(col("status") == "completed")
        
        # Aggregate: Total revenue by day
        daily_revenue = valid_orders \
            .groupBy("date") \
            .agg(
                sum("amount").alias("total_revenue"),
                count("*").alias("order_count"),
                avg("amount").alias("avg_order_value")
            )
        
        # Join with user activity logs
        enriched = valid_orders.join(
            logs,
            valid_orders.user_id == logs.user_id,
            "left"
        )
        
        return daily_revenue, enriched
    
    def load(self, daily_revenue, enriched):
        """Load: Write to destination"""
        
        # Write to data warehouse
        daily_revenue.write \
            .mode("overwrite") \
            .parquet("hdfs://warehouse/daily_revenue")
        
        # Write to analytics database
        enriched.write \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://localhost/analytics") \
            .option("dbtable", "order_analytics") \
            .mode("append") \
            .save()
    
    def run(self):
        """Run entire pipeline"""
        print("Starting batch pipeline...")
        
        # ETL
        orders, logs = self.extract()
        daily_revenue, enriched = self.transform(orders, logs)
        self.load(daily_revenue, enriched)
        
        print("Pipeline completed!")

# Run pipeline
pipeline = BatchPipeline()
pipeline.run()

# Benefits:
# âœ… Parallel processing (distributed)
# âœ… Fault tolerance (Spark)
# âœ… Scalable (add more nodes)
# âœ… Reusable (run daily)
```

### âœ… Stream Processing (Kafka Streams)

```python
# ===== STREAM PROCESSING (KAFKA STREAMS) =====

from kafka import KafkaConsumer, KafkaProducer
import json
from collections import defaultdict
from datetime import datetime, timedelta

class StreamProcessor:
    """Real-time stream processing"""
    
    def __init__(self):
        # Kafka consumer
        self.consumer = KafkaConsumer(
            'user_clicks',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        # Kafka producer (output)
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        # State (in-memory window)
        self.window_state = defaultdict(int)
        self.window_size = timedelta(minutes=5)
    
    def process_event(self, event):
        """Process single event"""
        
        # Extract fields
        user_id = event.get('user_id')
        page = event.get('page')
        timestamp = datetime.fromisoformat(event.get('timestamp'))
        
        # Filter: Only product pages
        if not page.startswith('/product/'):
            return None
        
        # Transform: Extract product_id
        product_id = page.split('/')[-1]
        
        # Aggregate: Count clicks per product (5-min window)
        window_key = f"{product_id}:{timestamp.strftime('%Y%m%d%H%M')}"
        self.window_state[window_key] += 1
        
        # Emit aggregated result
        result = {
            'product_id': product_id,
            'click_count': self.window_state[window_key],
            'window_start': timestamp.isoformat(),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        return result
    
    def start(self):
        """Start stream processing"""
        print("Starting stream processor...")
        
        for message in self.consumer:
            # Process event
            event = message.value
            result = self.process_event(event)
            
            if result:
                # Publish result
                self.producer.send('product_clicks', value=result)
                print(f"Processed: {result}")

# Run stream processor
processor = StreamProcessor()
processor.start()

# Benefits:
# âœ… Real-time (< 1 second latency)
# âœ… Continuous processing
# âœ… Stateful (maintains windows)
# âœ… Scalable (add more consumers)
```

### âœ… Production Pipeline (Airflow + Spark)

```python
# ===== PRODUCTION PIPELINE (AIRFLOW) =====

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

# Define DAG
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 1),
    'email': ['alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'daily_analytics_pipeline',
    default_args=default_args,
    description='Daily analytics ETL pipeline',
    schedule_interval='0 1 * * *',  # Run at 1 AM daily
    catchup=False
)

# Task 1: Extract from database
def extract_orders(**context):
    """Extract orders from database"""
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder.appName("Extract").getOrCreate()
    
    orders = spark.read \
        .format("jdbc") \
        .option("url", "jdbc:postgresql://localhost/shop") \
        .option("dbtable", "orders") \
        .option("user", "readonly") \
        .load()
    
    # Save to HDFS
    orders.write.parquet("hdfs://staging/orders", mode="overwrite")
    
    return "Extracted {} orders".format(orders.count())

extract_task = PythonOperator(
    task_id='extract_orders',
    python_callable=extract_orders,
    dag=dag
)

# Task 2: Transform data
def transform_orders(**context):
    """Transform and aggregate"""
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, sum, count
    
    spark = SparkSession.builder.appName("Transform").getOrCreate()
    
    orders = spark.read.parquet("hdfs://staging/orders")
    
    # Aggregate
    analytics = orders \
        .groupBy("user_id") \
        .agg(
            count("*").alias("order_count"),
            sum("amount").alias("total_spent")
        )
    
    # Save
    analytics.write.parquet("hdfs://staging/analytics", mode="overwrite")
    
    return "Transformed {} users".format(analytics.count())

transform_task = PythonOperator(
    task_id='transform_orders',
    python_callable=transform_orders,
    dag=dag
)

# Task 3: Load to warehouse
load_task = BashOperator(
    task_id='load_to_warehouse',
    bash_command='spark-submit load_to_warehouse.py',
    dag=dag
)

# Task 4: Update dashboard
def update_dashboard(**context):
    """Refresh dashboard"""
    import requests
    
    response = requests.post('http://dashboard/api/refresh')
    return f"Dashboard updated: {response.status_code}"

dashboard_task = PythonOperator(
    task_id='update_dashboard',
    python_callable=update_dashboard,
    dag=dag
)

# Define dependencies
extract_task >> transform_task >> load_task >> dashboard_task

# Benefits:
# âœ… Scheduled execution (daily)
# âœ… Error handling (retries)
# âœ… Monitoring (emails)
# âœ… Dependencies (task order)
# âœ… Production-ready
```

---

## ğŸ’¡ Mini Project: "Build Data Pipeline"

### Phase 1: Simple Batch Pipeline â­

**Requirements:**
- Extract from CSV
- Transform (clean, aggregate)
- Load to database
- Schedule with cron

---

### Phase 2: Spark Pipeline â­â­

**Requirements:**
- Distributed processing
- Multiple data sources
- Complex transformations
- Partitioned output

---

### Phase 3: Real-Time Stream â­â­â­

**Requirements:**
- Kafka stream processing
- Windowed aggregations
- State management
- Fault tolerance

---

## âš–ï¸ Batch vs Stream Comparison

| Aspect | Batch | Stream |
|--------|-------|--------|
| **Latency** | Hours | Seconds |
| **Complexity** | Simple | Complex |
| **Cost** | Low | High |
| **Use Case** | Reports | Real-time |
| **Volume** | Large | Continuous |
| **Debugging** | Easy | Hard |
| **Fault Tolerance** | Retry | Checkpointing |

---

## âŒ Common Mistakes

### Mistake 1: No Idempotency

```python
# âŒ Pipeline not idempotent
def process():
    orders = read_orders()
    analytics.insert(orders)  # Duplicate on rerun!

# âœ… Idempotent pipeline
def process():
    orders = read_orders()
    analytics.upsert(orders)  # Safe to rerun
```

### Mistake 2: No Backpressure

```python
# âŒ Consume faster than process
while True:
    event = consume()
    process(event)  # If slow: Queue fills, OOM!

# âœ… Backpressure handling
while queue.size() < MAX:
    event = consume()
    queue.add(event)
```

### Mistake 3: No Monitoring

```python
# âŒ No visibility into pipeline
process_data()

# âœ… Monitor metrics
metrics.gauge('pipeline.processed', count)
metrics.gauge('pipeline.latency', latency)
```

---

## ğŸ“š Additional Resources

**Batch:**
- [Apache Spark](https://spark.apache.org/)
- [Apache Airflow](https://airflow.apache.org/)

**Stream:**
- [Apache Kafka](https://kafka.apache.org/)
- [Apache Flink](https://flink.apache.org/)
- [Kafka Streams](https://kafka.apache.org/documentation/streams/)

---


## ğŸ¯ Before You Leave

**Can you answer these?**

1. **What's ETL?**
   - Answer: Extract, Transform, Load

2. **Batch vs stream?**
   - Answer: Batch = process later; Stream = process now

3. **What's windowing?**
   - Answer: Time-based grouping of events

4. **What's Lambda architecture?**
   - Answer: Batch + stream together

5. **When to use stream processing?**
   - Answer: Real-time requirements (fraud, alerts, monitoring)

**If you got these right, you're ready for the next topic!** âœ…

---

## ğŸ¤£ Closing Thoughts

> **Engineer:** "Let's process data in real-time!"
>
> **Manager:** "Why not batch at night?"
>
> **Engineer:** "Users want instant results!"
>
> **Manager:** "Build both then"
>
> **Engineer:** "Now I have two problems" ğŸ˜…

---

[â† Back to Main](../README.md) | [Previous: CQRS](30-cqrs.md) | [Next: Edge Computing â†’](32-edge-computing.md)

---

**Last Updated:** November 11, 2025  
**Difficulty:** â­â­â­â­ Advanced (distributed systems)  
**Time to Read:** 28 minutes  
**Time to Implement:** 6-10 hours per phase  

---

*Data pipelines: Moving data from A to B while transforming it into C. Simple, right?* ğŸš€